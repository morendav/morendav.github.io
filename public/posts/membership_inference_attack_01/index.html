<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Membership Inference Attack: Primer & Case Study | home</title><meta name=keywords content="Artificial Intelligence,Membership Inference Attack,Privacy,Case Study"><meta name=description content="Introduction to ML model memorization and privacy assessments using memcership inference attacks."><meta name=author content="D.Moreno"><link rel=canonical href=https://dlm.rocks/posts/membership_inference_attack_01/><link crossorigin=anonymous href=/assets/css/stylesheet.b183800e2cfbb62c3bce2b2ba56cdb2dd33af76c75cf4550173d5dfebd7c68a6.css integrity="sha256-sYOADiz7tiw7zisrpWzbLdM692x1z0VQFz1d/r18aKY=" rel="preload stylesheet" as=style><link rel=icon href=https://dlm.rocks/img/favicon.png><link rel=icon type=image/png sizes=16x16 href=https://dlm.rocks/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://dlm.rocks/favicon-32x32.png><link rel=apple-touch-icon href=https://dlm.rocks/apple-touch-icon.png><link rel=mask-icon href=https://dlm.rocks/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-H6P6WVSTW2"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-H6P6WVSTW2",{anonymize_ip:!1})}</script><meta property="og:title" content="Membership Inference Attack: Primer & Case Study"><meta property="og:description" content="Introduction to ML model memorization and privacy assessments using memcership inference attacks."><meta property="og:type" content="article"><meta property="og:url" content="https://dlm.rocks/posts/membership_inference_attack_01/"><meta property="og:image" content="https://dlm.rocks/papermod-cover.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-07-01T20:09:00-08:00"><meta property="article:modified_time" content="2024-12-25T20:54:50-08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://dlm.rocks/papermod-cover.png"><meta name=twitter:title content="Membership Inference Attack: Primer & Case Study"><meta name=twitter:description content="Introduction to ML model memorization and privacy assessments using memcership inference attacks."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://dlm.rocks/posts/"},{"@type":"ListItem","position":2,"name":"Membership Inference Attack: Primer \u0026 Case Study","item":"https://dlm.rocks/posts/membership_inference_attack_01/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Membership Inference Attack: Primer \u0026 Case Study","name":"Membership Inference Attack: Primer \u0026 Case Study","description":"Introduction to ML model memorization and privacy assessments using memcership inference attacks.","keywords":["Artificial Intelligence","Membership Inference Attack","Privacy","Case Study"],"articleBody":"Machine learning models depend on large troves of data to develop and improve their inference / prediction capabilities. These models generalize their abilities, but inevitably some of their training data is encoded or memorized within their trained parameters. This post explores a method of quantifying model memorization, factors that make models more prone to memorization, and provides some mitigations against memorization.\nüìå TL;DR ML models tend to memorize samples from their training data, this poses a privacy risk to members whose data was part of the training dataset Membership Inference Attacks (MIA) are assessments which may be used to quantify model memorization, these attacks can be performed either with full access to the model or with only access to the model‚Äôs predictions ML models learn from training data, with the goal of generalizing what they learn to new (unseen) samples. Despite this goal, Models generally perform better on their training datasets vs other similar data (e.g., data seen in the wild, or validation datasets) Models are said to be overfit when they perform significantly better on the training data compared to other samples Model output (vector of logits) are a signal that can be used by an attacker to deduce that sample‚Äôs participation in the training dataset Characteristics of the model and data affect model memorization In general Models with larger capacities tend to overfit sooner in training, and provide greater attacker advantage in memorization attacks Different model architectures yield different overfitting and memorization characteristics during training Datasets with more classes tend to train models with higher attacker advantage, this is because there is more information that the model exposes to an attacker (more signal for a memorization based attack) Datasets with lower entropy in a given class tend to train models with higher attacker advantage for that class See also the / Case Study / for results that agree (and some that don't) A method of assessing model privacy is the Membership Inference Attack (MIA) / MIA / seeks to predict whether a given sample was or was not part of a model‚Äôs training dataset MIA is fundamentally a classification task (sample was in, or sample was not in the training dataset) This classification task is well suited to a / binary classification model / , in this setup shadow models mimic the target model and an attacker model is trained on labeled samples and the shadow models‚Äô predictions and prediction metadata This can also be performed using / threshold attacks / where the model‚Äôs internal state is used to predict whether a sample was a part of the training dataset These methods are closed model and open model attacks respectively, in the former the model‚Äôs internal state is unknown to the attacker. Closed model attacks are generally of greater utility for attackers in the wild Why ML privacy matters (‚Ä¶and a bit of writer‚Äôs block) I found starting this post challenging‚Ä¶ the concern this post touches on is so foundational and broad that I found this all difficult to put into words (which is funny given my verbosity otherwise‚Ä¶) so I turned to an AI collaborator to help start things off: Bard.\nI punted a few questions Bard‚Äôs way, and iterated on the core concepts a bit before Bard helped me write this:\n\" Machine learning is a rapidly evolving field with the potential to revolutionize many aspects of our lives. It is already being used in a wide range of applications: from healthcare to finance to transportation. As machine learning continues to evolve, it is likely to become even more pervasive in our lives. These ML models are often trained on large datasets of data, which can include sensitive information.\nAs a result, the importance of machine learning is not just limited to its potential to improve our lives. It also has the potential to impact our privacy in significant ways. \"\nPrivacy \u0026 Machine Learning Machine learning algorithms have become ubiquitous across products we use everyday, these algorithms attempt to generalize while necessarily being trained on a subset of all possible examples.\nML Basics \u0026 Intuition This post is by no means an introduction to ML, there are much more skilled teachers that deliver the content in many forms of media - in particular I have enjoyed 3Blue1Brown‚Äôs series on YouTube. Some (simplified) basics for this post will do:\nGeneralizability\nConsider a classification task: an algorithm that is trained to output one of two labels (cat or dog) using images of dogs and cats as input. If this model is trained on all sorts of cats but only large dog breeds, then we can expect it to do well on images of all sorts of cats and only large breed dogs. Even without training, this model has pretty good odds of making the correct call, a 50/50 chance of making the correct prediction without having any trained ability to differentiate dogs and cats.\nLet‚Äôs say that after multiple epochs of training this same model is then tasked with classifying an image of a small breed dog that it has never seen: it may predict Dog with low confidence or worse‚Ä¶ predict Cat.\nThis intuitive example reveals a little bit about how machines are capable of learning: the model attempts to ‚Äúremember‚Äù a mapping from the inputs it has seen in the training data to the outputs, or the correct labels. In doing so, the model is able to map what it has seen during training to the output classes available (dog, cat). When presented with a new variant of a class it has seen then this mapping may not serve the model well.\nOverfitting\nContinuing this classification task example, the model is trained by:\nBeing shown a picture as input (dog, cat) Calculating the output prediction, by cascading neuron activations throughout it‚Äôs layers Comparing it‚Äôs prediction against the true value, calculating it‚Äôs error Using back propagation alchemy to tune it‚Äôs model weights and biases to better predict that specific example Rinse, repeat for all samples, repeat that over multiple epochs of training.\nBy following this procedure our classification model develops from an interconnected network of random parameters to a set of tuned parameters for its task. In general, many arbitrary ML algorithms follow this pattern: a simple regression algorithm begins with a series of random parameters and a deep neural network begins with random weights for each of its neurons and random biases for each of its connections. The process of training the model slowly tunes the trainable parameters of the model to fit its task based on the training examples that are used.\nComing back to the training process‚Ä¶ Let‚Äôs imagine this cycle runs indefinitely - repeatedly the model is exposed to those images of all sorts of cats and only large dogs. The same pictures of all sorts of cats, and only large breed dogs. Again. And again. And again.\nWe can expect that as this cycle approaches the end of time said model would get really good at identifying cats and large breed dogs. That is, the model would get very skilled at predicting THOSE all sorts of cats and THOSE large breed dogs it had seen in its training data (over and over and over and‚Ä¶)\nAfter each cycle of calculating losses and back propagating them through its parameters this model will overfit to the training data. In other words, the process will optimize the model‚Äôs trainable parameters to be very accurate for predicting data in it‚Äôs training dataset but will plateau or even develop this heightened accuracy at the expense of its ability to predict data outside of it‚Äôs training dataset.\nWhat this tells us about how machines learn Somewhere inside a trained model, within its weights and biases, this machine must retain some imprint or memory of the data it‚Äôs seen during its training. In general, models tend to perform better on samples from its training dataset, and if a model is overfit then this characteristic becomes exaggerated. The hope is that this model would generalize beyond the samples it had seen during training to new samples observed in the wild, but nonetheless its capabilities are learned during the training process.\nThis exploration of a model‚Äôs training and memory has profound implications for models trained on sensitive data, and especially in cases where even being a member of the training dataset reveals some sensitive characteristics about an individual.\nLet‚Äôs consider a more complicated model, a classification algorithm that predicts between a finite set of theparies for a cancer patient given some data points about the patient. If an attacker has access to an individual‚Äôs data and is able to deduce the patient‚Äôs participation in the training dataset then de facto the attacker has learned that this individual has cancer and the patient‚Äôs health privacy is violated.\nFor such an attack, the attacker wouldn‚Äôt need access to a model‚Äôs parameters, instead the attacker would only need to have access to the model‚Äôs predictions. This is the privacy vulnerability of a model that a Membership Inference Attack exploits.\nMeasuring Privacy of ML Models: MIA Membership Inference Attacks (MIA) were introduced by Shokri et al. 1 in 2017, and much of the research since its introduction has focused on improving the attack‚Äôs efficiency or proposing alternative methods to measure the same memorization tendencies of a model but the essence of the assessment remains the same: given a model and a datapoint, predict whether that datapoint was part of the model‚Äôs training dataset.\nLet‚Äôs explore two MIA approaches:\n1 - Membership Inference: Shadow Model Approach Earlier, our hypothetical model classified an image as being a member of one of two possible classes: Cat or Dog.\nNow we are posed with a question that also has a binary outcome: this datapoint was in or this datapoint was not in the training dataset used to train the target model. This classification task is well suited for another ML model that we will use as the attacker model.\nThis first MIA approach trains an attacker classification model to predict one of two possible outcomes (was in, was not in) for each class (in our example: cat, dog). This procedure is more involved, requiring more computation than the alternative because this method requires training multiple shadow ML models whose characteristics are similar to the target model we are attacking. This method also requires access to a known dataset that is similar to the target model‚Äôs training dataset, alternatively this known dataset may be generated from the target model. The shadow model approach at a high level involves two activities:\nTraining a set of shadow models that mirror the behavior of the target model\nThese shadow models use known samples as part of their training data We reserve some data beyond the training data to use as validation data Training an attacker classification model on the label (was in, was not in) and the outputs of the shadow models with the task of predicting whether a given datapoint represents a sample that was in that shadow model‚Äôs training dataset\nSince the training data for the shadow models are known, we can use these samples and the shadow models‚Äô predictions to train our classification model to recognize model outputs on training samples Samples from the training data fed through the shadow models are labeled as being a member, whereas samples from the validation dataset fed through the shadow models are labeled as being nonmember. These labels are the ground truth used for the attacker models prediction error and back propagation In Shokri et al. 1 demonstrated successful attacks using this approach with multiple shadow models. Random sampling at the onset of each shadow model‚Äôs training added entropy to the distribution of known data points into either of the training and test datasets. Later, Salem et al 2 demonstrated success using a single shadow model.\nThis MIA approach is often referred to as closed-system, closed-box, or black box MIA because direct access to the model‚Äôs parameters is not needed; instead only the target model‚Äôs output is needed (classification predictions, predictions confidence).\nReturning to our Cat-Dog example: Let us assume we have a set of known cat and dog images that we suspect is similar to the training data that was used to train the target model. These are the known images we will use to train our shadow models. Of this set of known samples, we split the images into a set of training and validation samples within each class, making sure that [cat, dog] are roughly equally represented in each of our split populations.\nNext, we train a shadow model that predicts cat or dog based on samples from our training data split.\nWe repeat these steps such that we have a set of shadow models, taking care to randomly assign data points to each model‚Äôs training dataset and validation dataset differently for each new shadow model we train.\nNext, we train the attacker classification model to predict one of [was in, was not in] labels.\nTo train the attacker model we present the shadow model with an image from its training dataset, use the model output as a feature for the attacker‚Äôs training data, and finally label that example as ‚Äòwas in‚Äô.\nRepeat this process for samples from that shadow model‚Äôs validation dataset (thus being outside of the shadow‚Äôs training data set), except the feature label for these samples are ‚Äòwas not in‚Äô\nBy following this methodology the attacker model is able to predict one of [was in, was not in] given a set of [model predictions, prediction metadata] generated by the shadow models. This process is repeated, training the same attacker model using the set of different shadow models to improve its ability to predict one of [was in, was not in] labels.\nAn attacker model is created in this way for each class in the output space.\nGenerating in-distribution training data In this hypothetical cat-dog example, we assume that we have access to a dataset that is (a) similar to the dataset used to train the target model and (b) contains data similar to the target datapoint we want to infer the membership of in the target model‚Äôs training dataset.\nFor an experimental set up or a proof of concept these assumptions are acceptable; however, in a practical setting they may be outside of what is available to us as an attacker. What can we do if we don‚Äôt have access to such a dataset? Or what if we are not confident that our known dataset is similar enough to the target dataset?\nIt is likely that in the wild a ‚Äòsimilar enough‚Äô dataset may not be available, in this case we can take advantage of the same model characteristics that are exploited by a MIA to generate in-distribution synthetic data. In this practical setting the only thing we may have confidence in or have access to are (1) the target model and (2) the target model‚Äôs output given some input. At the foundation of the MIA privacy assessment is this intuition that our classification model‚Äôs output label confidence is high for samples that are statistically similar to the model‚Äôs training dataset. This model behavior can be used to generate a set of images that are statistically representative of each class in the training dataset, iterating through each possible class in the output space this could be implemented in a number of ways including a two step algorithm described in 1\nSearch: Implement a hill climbing algorithm that attempts to maximize the model output‚Äôs classification confidence This process begins with a random initialization of all possible data points in the input feature space The features are randomly perturbed incrementally, and the gradient of the model‚Äôs output confidence is used to either promote to discard an iterations perturbations This process is repeated until a threshold confidence is reached Sampling: Once the hill-climb and incremental feature perturbations reach a threshold confidence in the model‚Äôs output, that sample is promoted to the synthetic dataset This is repeated with a different initialization until a sufficiently large synthetic dataset is generated A GANs variation of this approach trains adversarial generative and discriminative models to create more data within each class where we have some data but not enough, this augmentation may be used to create more data after some initial seed synthetic data is generated using the first approach.\n2 - Membership Inference: Threshold Approach This approach builds off of the original shadow model based approach with two additional insights:\nAs a shadow model‚Äôs characteristics converge on the target models they begin to overlap, in other words the most trusted shadow model is one that is an exact carbon copy of the target model\nThe same holds for the data we use to train the shadow models: if we have to generate our own synthetic data, the closer this becomes to mirror the original target model‚Äôs data, the better\nHere we fall back to these assertions, to simply use the target model as the shadow model and use the target model‚Äôs training and validation datasets instead of synthetic or similar datasets.\nThis method uses the target model‚Äôs predictions, prediction metadata (confidence, entropy, accuracy), intermediate model calculations, and ground truth labels for the data used to perform the MIA to assess the model‚Äôs capacity to memorize training examples 3 . These metrics are compared to a threshold value, and the algorithm counts how many data points from the training dataset and how many from the validation dataset are above the threshold values. Counting positive and negative membership predictions from both datasets (training and validation datasets) allows us to count true and false results using the ground truth memberships for positive predictions (the data point is in the training dataset) and negative predictions (the data point was not in the training dataset). Putting this together, we are able to generate a Receiver Operating Characteristic (ROC) curve which represents the accuracy with which an attacker could predict membership of a datapoint in the training dataset.\nComparing methods The threshold approach is less computationally demanding than the closed-box approach, and does not require any data generation since it uses the target model and its own training and test data to perform the MIA. Additionally, Song et al. 3 assert that the alternative (neural network based) MIA methods underestimate the privacy risk of memorized training examples.\nA keen privacy or security minded reader at this point may have tingling spidey senses: isn‚Äôt the harm here coming from an attacker having full access to the model and its data? If we have access to both, and have a target datapoint in mind, can‚Äôt we simply search that record in the dataset we have access to inorder to confirm membership? If we ground ourselves in the perspective of an attacker, this approach is not likely to reveal anything more valuable than what the attacker already has: the attacker would already have full open-box access to the model and have access to the entire training and validation datasets. With this grounding in mind we can conclude that this approach is useful to measure the memorization risk of a model theoretically but may not be as useful as an attack in the wild.\nThe shadow model approach inherits a dependency on how faithfully the shadow models‚Äô training and test datasets mirror the target model‚Äôs datasets, and how closely the shadow models‚Äô characteristics mirror that of the target model. Additionally, consider that this method seeks to train a classifier model using labeled examples and shadow models predictions and predication metadata: this system‚Äôs objective is to minimize the loss of its predictions (this sample was in, or this sample was not in the target model‚Äôs training dataset). This objective is symmetric, meaning losses for false positives and false negatives are equally weighted 4 . In practice an attacker isn‚Äôt concerned with false negatives (the attack fails to identify a sample as being a member of the training data) but a false positive (the attack predicts a sample is a member of the training data, but in fact it is not) significantly reduces the utility of the attack.\nNoteworthy improvements on the shadow model approach include listening to loss gradient as an additional signal for the MIA attacker model 5 .\nMitigations For brevity‚Äôs sake an exploration of these mitigations, and deeper dive will be left out of this post for some future content or as an exercise for the reader, a quick summary will do:\nDifferentially Private Stochastic Gradient Descent - a method of using differentially private model weight updates during training Coarsen model prediction confidence - limit the significant figures that the model reports along with it‚Äôs predictions, alternatively suppressing the confidence all together, this limits the signal available to a closed-system attack Adversarial Regularization 3 - train or fine-tune the target model and introduce MIA misdirection as part of the target model‚Äôs objective function Restrict predictions emitted from the target model to the top N classes (where N \u003c M total classes in the output space), this also reduces the signal available to an attacker MIA Case Study This case study‚Äôs code is hosted on GitHub.\nAn abridged version of this case study is also available as a Py notebook that you can experiment with.\nFirst, let‚Äôs dig into the library used to run the MIA and then we will get into the case study itself.\nLibrary Used: TensorFlow Privacy For this case study I elected to use the TensorFlow Privacy (TFP) APIs to run the MIA privacy assessments and report out the findings, the run_attacks method is passed at least three argument data objects:\nAttackType TFP membership inference attack API includes six different types of attacks\nTHRESHOLD_ENTROPY_ATTACK THRESHOLD_ATTACK LOGISTIC_REGRESSION MULTI_LAYERED_PERCEPTRON RANDOM_FOREST K_NEAREST_NEIGHBORS The first two are threshold based MIA assessments and the last four are shadow model based, for these last four the name of the AttackType implies the type of model that is built to perform the MIA.\nNote: MIA results are not directly comparable between attack types, instead the results should only be compared in relative terms.\nAttackInputData This data object stores information used for performing the MIA, this is data generated by the target model and is used by the attack model or threshold attack model to predict a sample‚Äôs membership in the target model‚Äôs training dataset. As a matter of configuration this structure can contain any of the following for both the train and validation datasets:\nGround truth labels Prediction losses Prediction entropy Either of the classification predictions‚Äô logits or probabilities SlicingSpec Each MIA can be performed with different slicing protocols, this data object specifies the dimensions along which the MIA results will be sliced.\nOptional Arguments All possible additional arguments can be found in the MIA repo‚Äôs datastructures.py, in this case study the following is also passed:\nPrivacyReportMetadata - is either generated during testing or passed from the model training history, and stores information about the model‚Äôs training (losses, accuracies, etc)\nCase Study \u0026 Results The case study runs three different experiments to demonstrate the dependence on model memorization on the following variables holding all else constant.\nThe canonical CIFAR10 dataset was used for these experiments, although experiment 3 applies a small twist to this 10 class dataset. The dataset is sourced from TensorFlow Datasets (thanks TFDS).\nCIFAR 10 Class Experiment Design Experiment 1 Trains two models with different architectures but similar ‚Äòdepth‚Äô\nThe models are a convolutional 2D model and a densely connected neural network model ‚ÄòDepth‚Äô is the number of repeated units in each model, for example a set of [a conv2d layer, and a pooling layer] are a single unit in the conv2d model. This unit is repeated N times, set at N=3 repeated units for both models in this experiment. Similar ‚Äòdepths‚Äô do not necessarily yield equal trainable parameters, reference the model summaries (also below) for models Conv2D (sequential) and Dense (sequential_1). The Dense NN has an order of magnitude greater parameter count. NOTE: The Conv2D model includes an additional Dense layer between the last unit (conv2d, pooling) and the logits. As a result, each Dense model appends an additional Dense layer to keep this model structure constant between these model types. In other words, a Dense model with 3 layers will actually have 4 Dense layers. Model: \"sequential\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 30, 30, 32) 896 max_pooling2d (MaxPooling2D (None, 15, 15, 32) 0 ) conv2d_1 (Conv2D) (None, 13, 13, 32) 9248 max_pooling2d_1 (MaxPooling (None, 6, 6, 32) 0 2D) conv2d_2 (Conv2D) (None, 4, 4, 32) 9248 max_pooling2d_2 (MaxPooling (None, 2, 2, 32) 0 2D) flatten (Flatten) (None, 128) 0 dense (Dense) (None, 64) 8256 dense_1 (Dense) (None, 10) 650 ================================================================= Total params: 28,298 Trainable params: 28,298 Non-trainable params: 0 _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten_1 (Flatten) (None, 3072) 0 dense_2 (Dense) (None, 64) 196672 dense_3 (Dense) (None, 64) 4160 dense_4 (Dense) (None, 64) 4160 dense_5 (Dense) (None, 64) 4160 dense_6 (Dense) (None, 10) 650 ================================================================= Total params: 209,802 Trainable params: 209,802 Non-trainable params: 0 Experiment 2 Uses two models with the same unit architecture, but different depths. In this experiment the Conv2D model is no longer used, instead two Dense NN are created with depths 3, and 6 (but actually 4 and 7, see note from experiment 1).\nOf these models, the 6 (i.e., 7) depth Dense NN has 222,282 trainable parameters, or an approximate 6% increase in the number of trainable parameters over, the 3 (i.e., 4) depth Dense NN.\n_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten_2 (Flatten) (None, 3072) 0 dense_7 (Dense) (None, 64) 196672 dense_8 (Dense) (None, 64) 4160 dense_9 (Dense) (None, 64) 4160 dense_10 (Dense) (None, 64) 4160 dense_11 (Dense) (None, 64) 4160 dense_12 (Dense) (None, 64) 4160 dense_13 (Dense) (None, 64) 4160 dense_14 (Dense) (None, 10) 650 ================================================================= Total params: 222,282 Trainable params: 222,282 Non-trainable params: 0 Experiment 3 Holds the model architectures constant, and trains two Dense NN models of equal depth and (approximately) the same number of trainable parameters. Instead the training and validation datasets are modified as this experiment explores the effect that the number of classes in the dataset has on MIA results.\nThese models have approximately the same number of trainable parameters, since the count(logits) must agree with the number of classes in the dataset, thus the 4 class, 3 Layer Dense NN has slightly lower count of trainable parameters (209,412) compared to the 10 class, 3 layer Dense NN (209,802), but this difference is negligible.\n_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten_3 (Flatten) (None, 3072) 0 dense_15 (Dense) (None, 64) 196672 dense_16 (Dense) (None, 64) 4160 dense_17 (Dense) (None, 64) 4160 dense_18 (Dense) (None, 64) 4160 dense_19 (Dense) (None, 4) 260 ================================================================= Total params: 209,412 Trainable params: 209,412 Non-trainable params: 0 _________________________________________________________________ Expectations \u0026 Results Before I ran the experiments I expected:\nThe model using repeated convolutional nn units should have a higher capacity of memorization I also expected this model to perform better at the classification task than the dense NN These are both consequences of the Conv2D model learning it‚Äôs filters parameters through the SGD process, these filters are better equipped to recognize features (i.e. memorize features) from our training dataset sooner in training More parameters should make the model overfit faster, and have a higher capacity of memorization A model‚Äôs ‚Äòcapacity‚Äô is a measure of the number of trainable parameters it has, intuitively this also means its memorization capacity is greater - it can simply encode more information than smaller models More classes in the datasets should mean the model‚Äôs MIA attacker advantage is higher (higher MIA results) In the closed model attack, having more classes in the output space means there is more signal for an attacker to use in the MIA Experiment 1 These results were as expected.\nThe training results demonstrate that the Conv2D layered model tends to overfit sooner, and overfit to a higher degree than the Dense NN model. This is suggested by model accuracy for the training dataset overtaking the accuracy for the validation dataset sooner (earlier in discrete epoch-timescale) and with a wider margin. The inverse is true for model loss as a matter of algebraic rearrangement.\nThis widening margin is reinforced in the MIA threshold attack results, where again the Conv2D model tends to memorize to a higher degree than its Dense counterpart.\nExperiment 1 - Training results Experiment 1 - MIA results Hypothesis\nConv2D units encode features within its filters either from the input image matrix or the preceding layers, these filters‚Äô values are learned during the training process. As a result, this architecture is able to encode characteristics of the input images much earlier in training than Dense connected NN are. I suspect this is the reason that despite having fewer trainable parameters (a whole order of magnitude less than the Dense NN model), the Conv2D model shows higher MIA results and faster overfitting rate.\nExperiment 2 These results were inconclusive.\nThis experiment proved inconclusive, the 6% increase in trainable parameters was a marginal increase that didn‚Äôt result in a material difference in model memorization or overfitting rate. The MIA results show approximately consistent memorization between these models.\nExperiment improvements: Another attempt at experiment 2 with a range of model depths (Depth = 1, 2, 4, 8, ‚Ä¶.) holding all else as constants may yield a better basis for comparison between the results for a range of model depths.\nExperiment 2 - Training results Experiment 2 - MIA results Experiment 3 These results did not align with my expectations.\nThe 10 class setup begins with lower accuracy in the training results plot, which represents the fact that an untrained model guessing blindly has a lower probability of getting the right answer for a 10 class dataset (probability of 10% that a blind guess is correct) than the 4 class variant (which has a 25% of a blind guess being correct).\nAfter training, the 4 class variant demonstrated higher MIA results and a widening margin between training and validation dataset prediction accuracies after the epoch when the model started overfitting.\nExperiment 3 - Training results Experiment 3 - MIA results Hypothesis\nConverting the CIFAR10 to be a 4 class dataset took a naive approach: select only samples from training and validation datasets where the label is one of the first four classes. The CIFAR10 dataset is evenly distributed across its labels, this means that the 4 class dataset has (2/5) the number of samples as the 10 class.\nHolding the model constant between the 10 and 4 class datasets means that there is significantly less data in each of the training and validation datasets for the 4 class dataset. It is possible that the effect of having significantly less data to work with made the 4class model overfit more and thus memorize the training samples to a higher degree than the 10class model.\nCIFAR 4 class dataset Reza Shokri, Marco Stronati, Congzheng Song, \u0026 Vitaly Shmatikov. (2017). Membership Inference Attacks against Machine Learning Models. https://arxiv.org/pdf/1610.05820.pdf¬†‚Ü©Ô∏é¬†‚Ü©Ô∏é¬†‚Ü©Ô∏é\nAhmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, \u0026 Michael Backes. (2018). ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models. https://arxiv.org/pdf/1806.01246.pdf¬†‚Ü©Ô∏é\nLiwei Song, \u0026 Prateek Mittal. (2020). Systematic Evaluation of Privacy Risks of Machine Learning Models. https://arxiv.org/pdf/2003.10595.pdf¬†‚Ü©Ô∏é¬†‚Ü©Ô∏é¬†‚Ü©Ô∏é\nNicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, \u0026 Florian Tramer. (2022). Membership Inference Attacks From First Principles. https://arxiv.org/pdf/2112.03570.pdf¬†‚Ü©Ô∏é\nYiyong Liu, Zhengyu Zhao, Michael Backes, \u0026 Yang Zhang. (2022). Membership Inference Attacks by Exploiting Loss Trajectory. https://arxiv.org/abs/2208.14933¬†‚Ü©Ô∏é\n","wordCount":"5256","inLanguage":"en","datePublished":"2023-07-01T20:09:00-08:00","dateModified":"2024-12-25T20:54:50-08:00","author":[{"@type":"Person","name":"D.Moreno"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://dlm.rocks/posts/membership_inference_attack_01/"},"publisher":{"@type":"Organization","name":"home","logo":{"@type":"ImageObject","url":"https://dlm.rocks/img/favicon.png"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://dlm.rocks/ accesskey=h title="home (Alt + H)">home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://dlm.rocks/archives title=archive><span>archive</span></a></li><li><a href=https://dlm.rocks/tags/ title=tags><span>tags</span></a></li><li><a href=https://dlm.rocks/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://dlm.rocks/about/ title=about><span>about</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://dlm.rocks/>Home</a>&nbsp;¬ª&nbsp;<a href=https://dlm.rocks/posts/>Posts</a></div><h1 class=post-title>Membership Inference Attack: Primer & Case Study</h1><div class=post-description>Introduction to ML model memorization and privacy assessments using memcership inference attacks.</div><div class=post-meta><span title='2023-07-01 20:09:00 -0800 -0800'>July 1, 2023</span>&nbsp;¬∑&nbsp;25 min&nbsp;¬∑&nbsp;D.Moreno</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#-tldr aria-label="üìå TL;DR">üìå TL;DR</a></li><li><a href=#why-ml-privacy-matters-and-a-bit-of-writers-block aria-label="Why ML privacy matters (&amp;hellip;and a bit of writer‚Äôs block)">Why ML privacy matters (&mldr;and a bit of writer‚Äôs block)</a></li><li><a href=#privacy--machine-learning aria-label="Privacy &amp;amp; Machine Learning">Privacy & Machine Learning</a><ul><li><a href=#ml-basics--intuition aria-label="ML Basics &amp;amp; Intuition">ML Basics & Intuition</a></li><li><a href=#what-this-tells-us-about-how-machines-learn aria-label="What this tells us about how machines learn">What this tells us about how machines learn</a></li></ul></li><li><a href=#measuring-privacy-of-ml-models-mia aria-label="Measuring Privacy of ML Models: MIA">Measuring Privacy of ML Models: MIA</a><ul><li><a href=#1---membership-inference-shadow-model-approach aria-label="1 - Membership Inference: Shadow Model Approach">1 - Membership Inference: Shadow Model Approach</a><ul><li><a href=#returning-to-our-cat-dog-example aria-label="Returning to our Cat-Dog example:">Returning to our Cat-Dog example:</a></li><li><a href=#generating-in-distribution-training-data aria-label="Generating in-distribution training data">Generating in-distribution training data</a></li></ul></li><li><a href=#2---membership-inference-threshold-approach aria-label="2 - Membership Inference: Threshold Approach">2 - Membership Inference: Threshold Approach</a></li><li><a href=#comparing-methods aria-label="Comparing methods">Comparing methods</a></li><li><a href=#mitigations aria-label=Mitigations>Mitigations</a></li></ul></li><li><a href=#mia-case-study aria-label="MIA Case Study">MIA Case Study</a><ul><li><a href=#library-used-tensorflow-privacy aria-label="Library Used: TensorFlow Privacy">Library Used: TensorFlow Privacy</a><ul><li><a href=#attacktype aria-label=AttackType>AttackType</a></li><li><a href=#attackinputdata aria-label=AttackInputData>AttackInputData</a></li><li><a href=#slicingspec aria-label=SlicingSpec>SlicingSpec</a></li><li><a href=#optional-arguments aria-label="Optional Arguments">Optional Arguments</a></li></ul></li><li><a href=#case-study--results aria-label="Case Study &amp;amp; Results">Case Study & Results</a><ul><li><a href=#experiment-design aria-label="Experiment Design">Experiment Design</a><ul><li><a href=#experiment-1 aria-label="Experiment 1"><strong>Experiment 1</strong></a></li></ul></li><li><a href=#experiment-2 aria-label="Experiment 2">Experiment 2</a><ul><li><a href=#experiment-3 aria-label="Experiment 3">Experiment 3</a></li></ul></li><li><a href=#expectations--results aria-label="Expectations &amp;amp; Results">Expectations & Results</a><ul><li><a href=#experiment-1-1 aria-label="Experiment 1">Experiment 1</a></li><li><a href=#experiment-2-1 aria-label="Experiment 2">Experiment 2</a></li><li><a href=#experiment-3-1 aria-label="Experiment 3">Experiment 3</a></li></ul></li></ul></li></ul></li></ul></div></details></div><div class=post-content><p>Machine learning models depend on large troves of data to develop and improve their inference / prediction capabilities. These models generalize their abilities, but inevitably some of their training data is encoded or memorized within their trained parameters. This post explores a method of quantifying model memorization, factors that make models more prone to memorization, and provides some mitigations against memorization.</p><hr><h1 id=-tldr>üìå TL;DR<a hidden class=anchor aria-hidden=true href=#-tldr>#</a></h1><ul><li>ML models tend to memorize samples from their training data, this poses a privacy risk to members whose data was part of the training dataset</li><li>Membership Inference Attacks (MIA) are assessments which may be used to quantify model memorization, these attacks can be performed either with full access to the model or with only access to the model&rsquo;s predictions</li></ul><table><tr><td><strong>ML models learn from training data, with the goal of generalizing what they learn to new (unseen) samples.</strong></td><td>Despite this goal,<ul><li>Models generally perform better on their training datasets vs other similar data (e.g., data seen in the wild, or validation datasets)<li>Models are said to be overfit when they perform significantly better on the training data compared to other samples<li>Model output (vector of logits) are a signal that can be used by an attacker to deduce that sample‚Äôs participation in the training dataset</li></ul></td></tr><tr><td><strong>Characteristics of the model and data affect model memorization</strong></td><td>In general<ul><li>Models with larger capacities tend to overfit sooner in training, and provide greater attacker advantage in memorization attacks<li>Different model architectures yield different overfitting and memorization characteristics during training<li>Datasets with more classes tend to train models with higher attacker advantage, this is because there is more information that the model exposes to an attacker (more signal for a memorization based attack)<li>Datasets with lower entropy in a given class tend to train models with higher attacker advantage for that class</li></ul>See also the <a href=/posts/membership_inference_attack_01/#mia_case_study>/ Case Study /</a> for results that agree (and some that don't)</td></tr><tr><td><strong>A method of assessing model privacy is the <span style=text-decoration:underline>Membership Inference Attack</span> (MIA)</strong></td><td><a href=/posts/membership_inference_attack_01/#mia_intro>/ MIA /</a> seeks to predict whether a given sample was or was not part of a model‚Äôs training dataset<p>MIA is fundamentally a classification task (sample was in, or sample was not in the training dataset)<ul><li>This classification task is well suited to a <a href=/posts/membership_inference_attack_01/#mia_intro_shadowmodel>/ binary classification model /</a> , in this setup shadow models mimic the target model and an attacker model is trained on labeled samples and the shadow models‚Äô predictions and prediction metadata<li>This can also be performed using <a href=/posts/membership_inference_attack_01/#mia_intro_threshold>/ threshold attacks /</a> where the model‚Äôs internal state is used to predict whether a sample was a part of the training dataset<li>These methods are closed model and open model attacks respectively, in the former the model‚Äôs internal state is unknown to the attacker. Closed model attacks are generally of greater utility for attackers in the wild</li></ul></td></tr></table><h1 id=why-ml-privacy-matters-and-a-bit-of-writers-block>Why ML privacy matters (&mldr;and a bit of writer‚Äôs block)<a hidden class=anchor aria-hidden=true href=#why-ml-privacy-matters-and-a-bit-of-writers-block>#</a></h1><p>I found starting this post challenging‚Ä¶ the concern this post touches on is so foundational and broad that I found this all difficult to put into words (which is funny given my verbosity otherwise‚Ä¶) so I turned to an AI collaborator to help start things off: Bard.</p><p>I punted a few questions Bard‚Äôs way, and iterated on the core concepts a bit before Bard helped me write this:</p><p>"
Machine learning is a rapidly evolving field with the potential to revolutionize many aspects of our lives. It is already being used in a wide range of applications: from healthcare to finance to transportation. As machine learning continues to evolve, it is likely to become even more pervasive in our lives. These ML models are often trained on large datasets of data, which can include sensitive information.</p><p>As a result, the importance of machine learning is not just limited to its potential to improve our lives. It also has the potential to impact our privacy in significant ways.
"</p><h1 id=privacy--machine-learning>Privacy & Machine Learning<a hidden class=anchor aria-hidden=true href=#privacy--machine-learning>#</a></h1><p>Machine learning algorithms have become ubiquitous across products we use everyday, these algorithms attempt to generalize while necessarily being trained on a subset of all possible examples.</p><h2 id=ml-basics--intuition>ML Basics & Intuition<a hidden class=anchor aria-hidden=true href=#ml-basics--intuition>#</a></h2><p>This post is by no means an introduction to ML, there are much more skilled teachers that deliver the content in many forms of media - in particular I have enjoyed <a href="https://www.youtube.com/watch?v=IHZwWFHWa-w">3Blue1Brown‚Äôs series on YouTube</a>. Some (simplified) basics for this post will do:</p><p><strong>Generalizability</strong></p><p>Consider a classification task: an algorithm that is trained to output one of two labels (cat or dog) using images of dogs and cats as input. If this model is trained on all sorts of cats but only large dog breeds, then we can expect it to do well on images of all sorts of cats and only large breed dogs. Even without training, this model has pretty good odds of making the correct call, a 50/50 chance of making the correct prediction without having any trained ability to differentiate dogs and cats.</p><p>Let‚Äôs say that after multiple epochs of training this same model is then tasked with classifying an image of a small breed dog that it has never seen: it may predict <em>Dog</em> with low confidence or worse‚Ä¶ predict <em>Cat</em>.</p><p>This intuitive example reveals a little bit about how machines are capable of learning: the model attempts to ‚Äúremember‚Äù a mapping from the inputs it has seen in the training data to the outputs, or the correct labels. In doing so, the model is able to map what it has seen during training to the output classes available (dog, cat). When presented with a new variant of a class <em>it has seen</em> then this mapping may not serve the model well.</p><p><strong>Overfitting</strong></p><p>Continuing this classification task example, the model is trained by:</p><ol><li>Being shown a picture as input (dog, cat)</li><li>Calculating the output prediction, by cascading neuron activations throughout it‚Äôs layers</li><li>Comparing it‚Äôs prediction against the true value, calculating it‚Äôs error</li><li>Using back propagation alchemy to tune it‚Äôs model weights and biases to better predict that specific example</li></ol><p>Rinse, repeat for all samples, repeat that over multiple epochs of training.</p><p>By following this procedure our classification model develops from an interconnected network of random parameters to a set of tuned parameters for its task. In general, many arbitrary ML algorithms follow this pattern: a simple regression algorithm begins with a series of random parameters and a deep neural network begins with random weights for each of its neurons and random biases for each of its connections. The process of training the model slowly tunes the <em>trainable parameters</em> of the model to fit its task based on the training examples that are used.</p><p>Coming back to the training process‚Ä¶ Let‚Äôs imagine this cycle runs indefinitely - repeatedly the model is exposed to those images of all sorts of cats and only large dogs. The same pictures of all sorts of cats, and only large breed dogs. Again. And again. And again.</p><p>We can expect that as this cycle approaches the end of time said model would get <em>really good</em> at identifying cats and large breed dogs. That is, the model would get very skilled at predicting <strong><em>THOSE</em></strong> all sorts of cats and <strong><em>THOSE</em></strong> large breed dogs it had seen in its training data (over and over and over and‚Ä¶)</p><p>After each cycle of calculating losses and back propagating them through its parameters this model will <strong>overfit</strong> to the training data. In other words, the process will optimize the model‚Äôs trainable parameters to be very accurate for predicting data in it‚Äôs training dataset but will plateau or even develop this heightened accuracy at the expense of its ability to predict data outside of it‚Äôs training dataset.</p><h2 id=what-this-tells-us-about-how-machines-learn>What this tells us about how machines learn<a hidden class=anchor aria-hidden=true href=#what-this-tells-us-about-how-machines-learn>#</a></h2><p>Somewhere inside a trained model, within its weights and biases, this machine must retain some imprint or memory of the data it‚Äôs seen during its training. In general, models tend to perform better on samples from its training dataset, and if a model is overfit then this characteristic becomes exaggerated. The hope is that this model would generalize beyond the samples it had seen during training to new samples observed in the wild, but nonetheless its capabilities are learned during the training process.</p><p>This exploration of a model‚Äôs training and memory has profound implications for models trained on sensitive data, and especially in cases where even being a member of the training dataset reveals some sensitive characteristics about an individual.</p><p>Let‚Äôs consider a more complicated model, a classification algorithm that predicts between a finite set of theparies for a cancer patient given some data points about the patient. If an attacker has access to an individual‚Äôs data and is able to deduce the patient‚Äôs participation in the training dataset then de facto the attacker has learned that this individual has cancer and the patient‚Äôs health privacy is violated.</p><p>For such an attack, the attacker wouldn‚Äôt need access to a model&rsquo;s parameters, instead the attacker would only need to have access to the model‚Äôs predictions. This is the privacy vulnerability of a model that a Membership Inference Attack exploits.</p><h1 id=measuring-privacy-of-ml-models-mia>Measuring Privacy of ML Models: MIA<a hidden class=anchor aria-hidden=true href=#measuring-privacy-of-ml-models-mia>#</a></h1><h4 id=mia_intro><a hidden class=anchor aria-hidden=true href=#mia_intro>#</a></h4><p>Membership Inference Attacks (MIA) were introduced by Shokri et al. <sub><cite><sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></cite></sub> in 2017, and much of the research since its introduction has focused on improving the attack‚Äôs efficiency or proposing alternative methods to measure the same memorization tendencies of a model but the essence of the assessment remains the same: given a model and a datapoint, predict whether that datapoint was part of the model‚Äôs training dataset.</p><p>Let‚Äôs explore two MIA approaches:</p><h2 id=1---membership-inference-shadow-model-approach>1 - Membership Inference: Shadow Model Approach<a hidden class=anchor aria-hidden=true href=#1---membership-inference-shadow-model-approach>#</a></h2><h4 id=mia_intro_shadowmodel><a hidden class=anchor aria-hidden=true href=#mia_intro_shadowmodel>#</a></h4><p>Earlier, our hypothetical model classified an image as being a member of one of two possible classes: Cat or Dog.</p><p>Now we are posed with a question that also has a binary outcome: this datapoint <em><span style=text-decoration:underline>was in</span></em> or this datapoint <em><span style=text-decoration:underline>was not in</span></em> the training dataset used to train the target model. This classification task is well suited for another ML model that we will use as the attacker model.</p><p>This first MIA approach trains an attacker classification model to predict one of two possible outcomes (was in, was not in) for each class (in our example: cat, dog). This procedure is more involved, requiring more computation than the alternative because this method requires training multiple shadow ML models whose characteristics are similar to the target model we are attacking. This method also requires access to a known dataset that is similar to the target model&rsquo;s training dataset, alternatively this <em>known dataset</em> may be <a href=/posts/membership_inference_attack_01/#mia_intro_shadowmodel_gendata>generated from the target model</a>. The <strong>shadow model approach</strong> at a high level involves two activities:</p><ol><li><p>Training a set of <strong>shadow models</strong> that mirror the behavior of the target model</p><ol><li>These shadow models use known samples as part of their training data</li><li>We reserve some data beyond the training data to use as validation data</li></ol></li><li><p>Training an <strong>attacker classification model</strong> on the label (was in, was not in) and the outputs of the shadow models with the task of predicting whether a given datapoint represents a sample that <span style=text-decoration:underline>was in that shadow model‚Äôs training dataset</span></p><ol><li>Since the training data for the shadow models are known, we can use these samples and the shadow models‚Äô predictions to train our classification model to recognize model outputs on training samples</li><li>Samples from the training data fed through the shadow models are labeled as being a member, whereas samples from the validation dataset fed through the shadow models are labeled as being nonmember. These labels are the ground truth used for the attacker models prediction error and back propagation</li></ol></li></ol><p>In Shokri et al. <sub><cite><sup id=fnref1:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></cite></sub> demonstrated successful attacks using this approach with multiple shadow models. Random sampling at the onset of each shadow model‚Äôs training added entropy to the distribution of known data points into either of the training and test datasets. Later, Salem et al <sub><cite><sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></cite></sub> demonstrated success using a single shadow model.</p><p>This MIA approach is often referred to as closed-system, closed-box, or black box MIA because direct access to the model‚Äôs parameters is not needed; instead only the target model‚Äôs output is needed (classification predictions, predictions confidence).</p><h3 id=returning-to-our-cat-dog-example>Returning to our Cat-Dog example:<a hidden class=anchor aria-hidden=true href=#returning-to-our-cat-dog-example>#</a></h3><p>Let us assume we have a set of known cat and dog images that we suspect is similar to the training data that was used to train the target model. These are the known images we will use to train our shadow models. Of this set of known samples, we split the images into a set of training and validation samples within each class, making sure that [cat, dog] are roughly equally represented in each of our split populations.</p><p>Next, we train a <strong>shadow model</strong> that predicts cat or dog based on samples from our training data split.</p><p>We repeat these steps such that we have a set of <strong>shadow models</strong>, taking care to randomly assign data points to each model&rsquo;s training dataset and validation dataset differently for each new <strong>shadow model</strong> we train.</p><p>Next, we train the <strong>attacker classification model</strong> to predict one of [was in, was not in] labels.</p><p>To train the attacker model we present the shadow model with an image from its training dataset, use the model output as a feature for the attacker‚Äôs training data, and finally label that example as ‚Äòwas in‚Äô.</p><p>Repeat this process for samples from that <strong>shadow model‚Äôs</strong> validation dataset (thus being outside of the shadow‚Äôs training data set), except the feature label for these samples are ‚Äòwas not in‚Äô</p><p>By following this methodology the <strong>attacker model</strong> is able to predict one of [was in, was not in] given a set of [model predictions, prediction metadata] generated by the <strong>shadow models</strong>. This process is repeated, training the same attacker model using the set of different shadow models to improve its ability to predict one of [was in, was not in] labels.</p><p>An <strong>attacker model</strong> is created in this way for each class in the output space.</p><h3 id=generating-in-distribution-training-data>Generating in-distribution training data<a hidden class=anchor aria-hidden=true href=#generating-in-distribution-training-data>#</a></h3><h4 id=mia_intro_shadowmodel_gendata><a hidden class=anchor aria-hidden=true href=#mia_intro_shadowmodel_gendata>#</a></h4><p>In this hypothetical cat-dog example, we assume that we have access to a dataset that is (a) similar to the dataset used to train the target model and (b) contains data similar to the target datapoint we want to infer the membership of in the target model‚Äôs training dataset.</p><p>For an experimental set up or a proof of concept these assumptions are acceptable; however, in a practical setting they may be outside of what is available to us as an attacker. What can we do if we don‚Äôt have access to such a dataset? Or what if we are not confident that our <em>known dataset</em> is similar enough to the target dataset?</p><p>It is likely that in the wild a ‚Äòsimilar enough‚Äô dataset may not be available, in this case we can take advantage of the same model characteristics that are exploited by a MIA to generate in-distribution synthetic data. In this practical setting the only thing we may have confidence in or have access to are (1) the target model and (2) the target model‚Äôs output given some input. At the foundation of the MIA privacy assessment is this intuition that our classification model‚Äôs output label confidence is high for samples that are statistically similar to the model‚Äôs training dataset. This model behavior can be used to generate a set of images that are statistically representative of each class in the training dataset, iterating through each possible class in the output space this could be implemented in a number of ways including a two step algorithm described in <sub><cite><sup id=fnref2:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></cite></sub></p><ol><li><strong>Search</strong>: Implement a hill climbing algorithm that attempts to maximize the model output‚Äôs classification confidence<ol><li>This process begins with a random initialization of all possible data points in the input feature space</li><li>The features are randomly perturbed incrementally, and the gradient of the model‚Äôs output confidence is used to either promote to discard an iterations perturbations</li><li>This process is repeated until a threshold confidence is reached</li></ol></li><li><strong>Sampling</strong>: Once the hill-climb and incremental feature perturbations reach a threshold confidence in the model‚Äôs output, that sample is promoted to the synthetic dataset<ol><li>This is repeated with a different initialization until a sufficiently large synthetic dataset is generated</li></ol></li></ol><p>A GANs variation of this approach trains adversarial generative and discriminative models to create more data within each class where we have some data but not enough, this augmentation may be used to create more data after some initial seed synthetic data is generated using the first approach.</p><h2 id=2---membership-inference-threshold-approach>2 - Membership Inference: Threshold Approach<a hidden class=anchor aria-hidden=true href=#2---membership-inference-threshold-approach>#</a></h2><h4 id=mia_intro_threshold><a hidden class=anchor aria-hidden=true href=#mia_intro_threshold>#</a></h4><p>This approach builds off of the original shadow model based approach with two additional insights:</p><ol><li><p>As a shadow model‚Äôs characteristics converge on the target models they begin to overlap, in other words the most trusted shadow model is one that is an exact carbon copy of the target model</p></li><li><p>The same holds for the data we use to train the shadow models: if we have to generate our own synthetic data, the closer this becomes to mirror the original target model‚Äôs data, the better</p></li></ol><p>Here we fall back to these assertions, to simply use the target model as the shadow model and use the target model‚Äôs training and validation datasets instead of synthetic or similar datasets.</p><p>This method uses the target model‚Äôs predictions, prediction metadata (confidence, entropy, accuracy), intermediate model calculations, and ground truth labels for the data used to perform the MIA to assess the model‚Äôs capacity to memorize training examples <sub><cite><sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup></cite></sub> . These metrics are compared to a threshold value, and the algorithm counts how many data points from the training dataset and how many from the validation dataset are above the threshold values. Counting positive and negative membership predictions from both datasets (training and validation datasets) allows us to count true and false results using the ground truth memberships for positive predictions (the data point is in the training dataset) and negative predictions (the data point was not in the training dataset). Putting this together, we are able to generate a Receiver Operating Characteristic (ROC) curve which represents the accuracy with which an attacker could predict membership of a datapoint in the training dataset.</p><h2 id=comparing-methods>Comparing methods<a hidden class=anchor aria-hidden=true href=#comparing-methods>#</a></h2><p><strong><span style=text-decoration:underline>The threshold approach</span></strong> is less computationally demanding than the closed-box approach, and does not require any data generation since it uses the target model and its own training and test data to perform the MIA. Additionally, Song et al. <sub><cite><sup id=fnref1:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup></cite></sub> assert that the alternative (neural network based) MIA methods underestimate the privacy risk of memorized training examples.</p><p>A keen privacy or security minded reader at this point may have tingling spidey senses: isn‚Äôt the harm here coming from an attacker having full access to the model and its data? If we have access to both, and have a target datapoint in mind, can‚Äôt we simply search that record in the dataset we have access to inorder to confirm membership? If we ground ourselves in the perspective of an attacker, this approach is not likely to reveal anything more valuable than what the attacker already has: the attacker would already have full open-box access to the model and have access to the entire training and validation datasets. With this grounding in mind we can conclude that this approach is useful to measure the memorization risk of a model theoretically but may not be as useful as an attack in the wild.</p><p><strong><span style=text-decoration:underline>The shadow model approach</span></strong> inherits a dependency on how faithfully the shadow models‚Äô training and test datasets mirror the target model‚Äôs datasets, and how closely the shadow models‚Äô characteristics mirror that of the target model. Additionally, consider that this method seeks to train a classifier model using labeled examples and shadow models predictions and predication metadata: this system‚Äôs objective is to minimize the loss of its predictions (this sample was in, or this sample was not in the target model‚Äôs training dataset). This objective is symmetric, meaning losses for false positives and false negatives are equally weighted <sub><cite><sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup></cite></sub> . In practice an attacker isn‚Äôt concerned with false negatives (the attack fails to identify a sample as being a member of the training data) but a false positive (the attack predicts a sample is a member of the training data, but in fact it is not) significantly reduces the utility of the attack.</p><p>Noteworthy improvements on the shadow model approach include listening to loss gradient as an additional signal for the MIA attacker model <sub><cite><sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup></cite></sub> .</p><h2 id=mitigations>Mitigations<a hidden class=anchor aria-hidden=true href=#mitigations>#</a></h2><p>For brevity‚Äôs sake an exploration of these mitigations, and deeper dive will be left out of this post for some future content or as an exercise for the reader, a quick summary will do:</p><ul><li>Differentially Private Stochastic Gradient Descent - a method of using differentially private model weight updates during training</li><li>Coarsen model prediction confidence - limit the significant figures that the model reports along with it‚Äôs predictions, alternatively suppressing the confidence all together, this limits the signal available to a closed-system attack</li><li>Adversarial Regularization <sub><cite><sup id=fnref2:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup></cite></sub> - train or fine-tune the target model and introduce MIA misdirection as part of the target model‚Äôs objective function</li><li>Restrict predictions emitted from the target model to the top N classes (where N &lt; M total classes in the output space), this also reduces the signal available to an attacker</li></ul><h1 id=mia-case-study>MIA Case Study<a hidden class=anchor aria-hidden=true href=#mia-case-study>#</a></h1><p><a href=https://github.com/morendav/mia_image_classifier/blob/main/mia_intro_imageclassifier.py#L425>This case study‚Äôs code is hosted on GitHub.</a></p><p>An abridged version of this case study is also available as a <a href=https://github.com/morendav/mia_image_classifier/blob/main/mia_image_classifier_companion_notebook.ipynb>Py notebook</a> that you can experiment with.</p><p>First, let‚Äôs dig into the library used to run the MIA and then we will get into the case study itself.</p><h2 id=library-used-tensorflow-privacy>Library Used: TensorFlow Privacy<a hidden class=anchor aria-hidden=true href=#library-used-tensorflow-privacy>#</a></h2><p>For this case study I elected to use the <a href=https://github.com/tensorflow/privacy/blob/master/tensorflow_privacy/privacy/privacy_tests/membership_inference_attack/README.md>TensorFlow Privacy (TFP)</a> APIs to run the MIA privacy assessments and report out the findings, the run_attacks method is passed at least three argument data objects:</p><h3 id=attacktype>AttackType<a hidden class=anchor aria-hidden=true href=#attacktype>#</a></h3><p>TFP membership inference attack API includes <a href=https://github.com/tensorflow/privacy/blob/master/tensorflow_privacy/privacy/privacy_tests/membership_inference_attack/data_structures.py#L155>six different types of attacks</a></p><ol><li>THRESHOLD_ENTROPY_ATTACK</li><li>THRESHOLD_ATTACK</li><li>LOGISTIC_REGRESSION</li><li>MULTI_LAYERED_PERCEPTRON</li><li>RANDOM_FOREST</li><li>K_NEAREST_NEIGHBORS</li></ol><p>The first two are threshold based MIA assessments and the last four are shadow model based, for these last four the name of the AttackType implies the type of model that is built to perform the MIA.</p><p><em>Note: MIA results are not directly comparable between attack types, instead the results should only be compared in relative terms.</em></p><h3 id=attackinputdata>AttackInputData<a hidden class=anchor aria-hidden=true href=#attackinputdata>#</a></h3><p>This <a href=https://github.com/tensorflow/privacy/blob/45da453410ffa078b2d05dc4883d006d578e1b6d/tensorflow_privacy/privacy/privacy_tests/membership_inference_attack/data_structures.py#L228>data object</a> stores information used for performing the MIA, this is data generated by the target model and is used by the attack model or threshold attack model to predict a sample‚Äôs membership in the target model‚Äôs training dataset. As a matter of configuration this structure can contain any of the following for both the train and validation datasets:</p><ul><li>Ground truth labels</li><li>Prediction losses</li><li>Prediction entropy</li><li>Either of the classification predictions‚Äô logits or probabilities</li></ul><h3 id=slicingspec>SlicingSpec<a hidden class=anchor aria-hidden=true href=#slicingspec>#</a></h3><p>Each MIA can be performed with different slicing protocols, <a href=https://github.com/tensorflow/privacy/blob/45da453410ffa078b2d05dc4883d006d578e1b6d/tensorflow_privacy/privacy/privacy_tests/membership_inference_attack/data_structures.py#L78>this data object</a> specifies the dimensions along which the MIA results will be sliced.</p><h3 id=optional-arguments>Optional Arguments<a hidden class=anchor aria-hidden=true href=#optional-arguments>#</a></h3><p>All possible additional arguments can be found in the MIA repo‚Äôs <a href=https://github.com/tensorflow/privacy/blob/master/tensorflow_privacy/privacy/privacy_tests/membership_inference_attack/data_structures.py#L78>datastructures.py</a>, in this case study the following is also passed:</p><p><strong>PrivacyReportMetadata</strong> - is either generated during testing or passed from the model training history, and stores information about the <a href=https://github.com/tensorflow/privacy/blob/45da453410ffa078b2d05dc4883d006d578e1b6d/tensorflow_privacy/privacy/privacy_tests/membership_inference_attack/data_structures.py#L916>model‚Äôs training (losses, accuracies, etc)</a></p><h2 id=case-study--results>Case Study & Results<a hidden class=anchor aria-hidden=true href=#case-study--results>#</a></h2><h4 id=mia_case_study><a hidden class=anchor aria-hidden=true href=#mia_case_study>#</a></h4><p>The case study runs three different experiments to demonstrate the dependence on model memorization on the following variables holding all else constant.</p><p>The canonical CIFAR10 dataset was used for these experiments, although experiment 3 applies a small twist to this 10 class dataset. The dataset is sourced from TensorFlow Datasets (thanks TFDS).</p><figure class=align-center><a href=https://github.com/morendav/mia_image_classifier/blob/main/Experiment_Report/dataset_10class_cifar.png target=_blank><img loading=lazy src=https://raw.githubusercontent.com/morendav/mia_image_classifier/main/Experiment_Report/dataset_10class_cifar.png></a><figcaption>CIFAR 10 Class</figcaption></figure><h3 id=experiment-design>Experiment Design<a hidden class=anchor aria-hidden=true href=#experiment-design>#</a></h3><h4 id=experiment-1><strong>Experiment 1</strong><a hidden class=anchor aria-hidden=true href=#experiment-1>#</a></h4><p>Trains two models with different architectures but similar ‚Äòdepth‚Äô</p><ul><li>The models are a convolutional 2D model and a densely connected neural network model</li><li>‚ÄòDepth‚Äô is the number of repeated units in each model, for example a set of [a conv2d layer, and a pooling layer] are a single unit in the conv2d model. This unit is repeated N times, set at N=3 repeated units for both models in this experiment.<ul><li>Similar ‚Äòdepths‚Äô do not necessarily yield equal trainable parameters, reference the <a href=https://github.com/morendav/mia_image_classifier/blob/main/Experiment_Report/console.txt>model summaries</a> (also below) for models Conv2D (sequential) and Dense (sequential_1). The Dense NN has an order of magnitude greater parameter count.</li><li>NOTE: The Conv2D model includes an additional Dense layer between the last unit (conv2d, pooling) and the logits. As a result, each Dense model appends an additional Dense layer to keep this model structure constant between these model types. In other words, a Dense model with 3 layers will actually have 4 Dense layers.</li></ul></li></ul><pre tabindex=0><code>Model: &#34;sequential&#34;
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 conv2d (Conv2D)             (None, 30, 30, 32)        896

 max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0
 )

 conv2d_1 (Conv2D)           (None, 13, 13, 32)        9248

 max_pooling2d_1 (MaxPooling  (None, 6, 6, 32)         0
 2D)

 conv2d_2 (Conv2D)           (None, 4, 4, 32)          9248

 max_pooling2d_2 (MaxPooling  (None, 2, 2, 32)         0
 2D)

 flatten (Flatten)           (None, 128)               0

 dense (Dense)               (None, 64)                8256

 dense_1 (Dense)             (None, 10)                650

=================================================================
Total params: 28,298
Trainable params: 28,298
Non-trainable params: 0
</code></pre><pre tabindex=0><code>_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 flatten_1 (Flatten)         (None, 3072)              0

 dense_2 (Dense)             (None, 64)                196672

 dense_3 (Dense)             (None, 64)                4160

 dense_4 (Dense)             (None, 64)                4160

 dense_5 (Dense)             (None, 64)                4160

 dense_6 (Dense)             (None, 10)                650

=================================================================
Total params: 209,802
Trainable params: 209,802
Non-trainable params: 0
</code></pre><h3 id=experiment-2>Experiment 2<a hidden class=anchor aria-hidden=true href=#experiment-2>#</a></h3><p>Uses two models with the same unit architecture, but different depths. In this experiment the Conv2D model is no longer used, instead two Dense NN are created with depths 3, and 6 (but actually 4 and 7, see note from experiment 1).</p><p>Of these models, the 6 (i.e., 7) depth Dense NN has 222,282 trainable parameters, or an approximate 6% increase in the number of trainable parameters over, the 3 (i.e., 4) depth Dense NN.</p><pre tabindex=0><code>_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 flatten_2 (Flatten)         (None, 3072)              0

 dense_7 (Dense)             (None, 64)                196672

 dense_8 (Dense)             (None, 64)                4160

 dense_9 (Dense)             (None, 64)                4160

 dense_10 (Dense)            (None, 64)                4160

 dense_11 (Dense)            (None, 64)                4160

 dense_12 (Dense)            (None, 64)                4160

 dense_13 (Dense)            (None, 64)                4160

 dense_14 (Dense)            (None, 10)                650

=================================================================
Total params: 222,282
Trainable params: 222,282
Non-trainable params: 0
</code></pre><h4 id=experiment-3>Experiment 3<a hidden class=anchor aria-hidden=true href=#experiment-3>#</a></h4><p>Holds the model architectures constant, and trains two Dense NN models of equal depth and (approximately) the same number of trainable parameters. Instead the training and validation datasets are modified as this experiment explores the effect that the number of classes in the dataset has on MIA results.</p><p>These models have <em>approximately</em> the same number of trainable parameters, since the count(logits) must agree with the number of classes in the dataset, thus the 4 class, 3 Layer Dense NN has slightly lower count of trainable parameters (209,412) compared to the 10 class, 3 layer Dense NN (209,802), but this difference is negligible.</p><pre tabindex=0><code>_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 flatten_3 (Flatten)         (None, 3072)              0

 dense_15 (Dense)            (None, 64)                196672

 dense_16 (Dense)            (None, 64)                4160

 dense_17 (Dense)            (None, 64)                4160

 dense_18 (Dense)            (None, 64)                4160

 dense_19 (Dense)            (None, 4)                 260

=================================================================
Total params: 209,412
Trainable params: 209,412
Non-trainable params: 0
_________________________________________________________________
</code></pre><h3 id=expectations--results>Expectations & Results<a hidden class=anchor aria-hidden=true href=#expectations--results>#</a></h3><p>Before I ran the experiments I expected:</p><ul><li>The model using repeated convolutional nn units should have a higher capacity of memorization<ul><li>I also expected this model to perform better at the classification task than the dense NN</li><li>These are both consequences of the Conv2D model learning it‚Äôs filters parameters through the SGD process, these filters are better equipped to recognize features (i.e. memorize features) from our training dataset sooner in training</li></ul></li><li>More parameters should make the model overfit faster, and have a higher capacity of memorization<ul><li>A model‚Äôs ‚Äòcapacity‚Äô is a measure of the number of trainable parameters it has, intuitively this also means its memorization capacity is greater - it can simply encode more information than smaller models</li></ul></li><li>More classes in the datasets should mean the model‚Äôs MIA attacker advantage is higher (higher MIA results)<ul><li>In the closed model attack, having more classes in the output space means there is more signal for an attacker to use in the MIA</li></ul></li></ul><h4 id=experiment-1-1>Experiment 1<a hidden class=anchor aria-hidden=true href=#experiment-1-1>#</a></h4><p><strong>These results were as expected.</strong></p><p>The training results demonstrate that the Conv2D layered model tends to overfit sooner, and overfit to a higher degree than the Dense NN model. This is suggested by model accuracy for the training dataset overtaking the accuracy for the validation dataset sooner (earlier in discrete epoch-timescale) and with a wider margin. The inverse is true for model loss as a matter of algebraic rearrangement.</p><p>This widening margin is reinforced in the MIA threshold attack results, where again the Conv2D model tends to memorize to a higher degree than its Dense counterpart.</p><figure class=align-center><a href=https://github.com/morendav/mia_image_classifier/blob/main/Experiment_Report/expModelType_training_results.png target=_blank><img loading=lazy src=https://raw.githubusercontent.com/morendav/mia_image_classifier/main/Experiment_Report/expModelType_training_results.png></a><figcaption>Experiment 1 - Training results</figcaption></figure><figure class=align-center><a href=https://github.com/morendav/mia_image_classifier/blob/main/Experiment_Report/expModelType_mia_results.png target=_blank><img loading=lazy src=https://raw.githubusercontent.com/morendav/mia_image_classifier/main/Experiment_Report/expModelType_mia_results.png></a><figcaption>Experiment 1 - MIA results</figcaption></figure><p><strong>Hypothesis</strong></p><p>Conv2D units encode features within its filters either from the input image matrix or the preceding layers, these filters‚Äô values are learned during the training process. As a result, this architecture is able to encode characteristics of the input images much earlier in training than Dense connected NN are. I suspect this is the reason that despite having fewer trainable parameters (a whole order of magnitude less than the Dense NN model), the Conv2D model shows higher MIA results and faster overfitting rate.</p><h4 id=experiment-2-1>Experiment 2<a hidden class=anchor aria-hidden=true href=#experiment-2-1>#</a></h4><p><strong>These results were inconclusive</strong>.</p><p>This experiment proved inconclusive, the 6% increase in trainable parameters was a marginal increase that didn‚Äôt result in a material difference in model memorization or overfitting rate. The MIA results show approximately consistent memorization between these models.</p><p>Experiment improvements: Another attempt at experiment 2 with a range of model depths (Depth = 1, 2, 4, 8, ‚Ä¶.) holding all else as constants may yield a better basis for comparison between the results for a range of model depths.</p><figure class=align-center><a href=https://github.com/morendav/mia_image_classifier/blob/main/Experiment_Report/expDepth_training_results.png target=_blank><img loading=lazy src=https://raw.githubusercontent.com/morendav/mia_image_classifier/main/Experiment_Report/expDepth_training_results.png></a><figcaption>Experiment 2 - Training results</figcaption></figure><figure class=align-center><a href=https://github.com/morendav/mia_image_classifier/blob/main/Experiment_Report/expDepth_mia_results.png target=_blank><img loading=lazy src=https://raw.githubusercontent.com/morendav/mia_image_classifier/main/Experiment_Report/expDepth_mia_results.png></a><figcaption>Experiment 2 - MIA results</figcaption></figure><h4 id=experiment-3-1>Experiment 3<a hidden class=anchor aria-hidden=true href=#experiment-3-1>#</a></h4><p><strong>These results did not align with my expectations</strong>.</p><p>The 10 class setup begins with lower accuracy in the training results plot, which represents the fact that an untrained model guessing blindly has a lower probability of getting the right answer for a 10 class dataset (probability of 10% that a blind guess is correct) than the 4 class variant (which has a 25% of a blind guess being correct).</p><p>After training, the 4 class variant demonstrated higher MIA results and a widening margin between training and validation dataset prediction accuracies after the epoch when the model started overfitting.</p><figure class=align-center><a href=https://github.com/morendav/mia_image_classifier/blob/main/Experiment_Report/exp10C_4C_training_results.png target=_blank><img loading=lazy src=https://raw.githubusercontent.com/morendav/mia_image_classifier/main/Experiment_Report/exp10C_4C_training_results.png></a><figcaption>Experiment 3 - Training results</figcaption></figure><figure class=align-center><a href=https://github.com/morendav/mia_image_classifier/blob/main/Experiment_Report/exp10C_4C_mia_results.png target=_blank><img loading=lazy src=https://raw.githubusercontent.com/morendav/mia_image_classifier/main/Experiment_Report/exp10C_4C_mia_results.png></a><figcaption>Experiment 3 - MIA results</figcaption></figure><p><strong>Hypothesis</strong></p><p>Converting the CIFAR10 to be a 4 class dataset <a href=https://github.com/morendav/mia_image_classifier/blob/6ac2d10ae4b1302e0fce5cd71c929ea1c22b82ae/mia_intro_imageclassifier.py#L152>took a naive approach</a>: select only samples from training and validation datasets where the label is one of the first four classes. The CIFAR10 dataset is evenly distributed across its labels, this means that the 4 class dataset has (2/5) the number of samples as the 10 class.</p><p>Holding the model constant between the 10 and 4 class datasets means that there is significantly less data in each of the training and validation datasets for the 4 class dataset. It is possible that the effect of having significantly less data to work with made the 4class model overfit more and thus memorize the training samples to a higher degree than the 10class model.</p><figure class=align-center><a href=https://github.com/morendav/mia_image_classifier/blob/main/Experiment_Report/dataset_4class_cifar.png target=_blank><img loading=lazy src=https://raw.githubusercontent.com/morendav/mia_image_classifier/main/Experiment_Report/dataset_4class_cifar.png></a><figcaption>CIFAR 4 class dataset</figcaption></figure><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Reza Shokri, Marco Stronati, Congzheng Song, & Vitaly Shmatikov. (2017). Membership Inference Attacks against Machine Learning Models. <a href=https://arxiv.org/pdf/1610.05820.pdf>https://arxiv.org/pdf/1610.05820.pdf</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, & Michael Backes. (2018). ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models. <a href=https://arxiv.org/pdf/1806.01246.pdf>https://arxiv.org/pdf/1806.01246.pdf</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>Liwei Song, & Prateek Mittal. (2020). Systematic Evaluation of Privacy Risks of Machine Learning Models. <a href=https://arxiv.org/pdf/2003.10595.pdf>https://arxiv.org/pdf/2003.10595.pdf</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, & Florian Tramer. (2022). Membership Inference Attacks From First Principles. <a href=https://arxiv.org/pdf/2112.03570.pdf>https://arxiv.org/pdf/2112.03570.pdf</a>&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>Yiyong Liu, Zhengyu Zhao, Michael Backes, & Yang Zhang. (2022). Membership Inference Attacks by Exploiting Loss Trajectory. <a href=https://arxiv.org/abs/2208.14933>https://arxiv.org/abs/2208.14933</a>&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://dlm.rocks/tags/artificial-intelligence/>Artificial Intelligence</a></li><li><a href=https://dlm.rocks/tags/membership-inference-attack/>Membership Inference Attack</a></li><li><a href=https://dlm.rocks/tags/privacy/>Privacy</a></li><li><a href=https://dlm.rocks/tags/case-study/>Case Study</a></li></ul><nav class=paginav><a class=next href=https://dlm.rocks/posts/ai_art_wins_art_contest/><span class=title>Next ¬ª</span><br><span>AI in Art & Creativity</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Membership Inference Attack: Primer & Case Study on twitter" href="https://twitter.com/intent/tweet/?text=Membership%20Inference%20Attack%3a%20Primer%20%26%20Case%20Study&amp;url=https%3a%2f%2fdlm.rocks%2fposts%2fmembership_inference_attack_01%2f&amp;hashtags=ArtificialIntelligence%2cMembershipInferenceAttack%2cPrivacy%2cCaseStudy"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Membership Inference Attack: Primer & Case Study on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fdlm.rocks%2fposts%2fmembership_inference_attack_01%2f&amp;title=Membership%20Inference%20Attack%3a%20Primer%20%26%20Case%20Study&amp;summary=Membership%20Inference%20Attack%3a%20Primer%20%26%20Case%20Study&amp;source=https%3a%2f%2fdlm.rocks%2fposts%2fmembership_inference_attack_01%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Membership Inference Attack: Primer & Case Study on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fdlm.rocks%2fposts%2fmembership_inference_attack_01%2f&title=Membership%20Inference%20Attack%3a%20Primer%20%26%20Case%20Study"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Membership Inference Attack: Primer & Case Study on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fdlm.rocks%2fposts%2fmembership_inference_attack_01%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Membership Inference Attack: Primer & Case Study on whatsapp" href="https://api.whatsapp.com/send?text=Membership%20Inference%20Attack%3a%20Primer%20%26%20Case%20Study%20-%20https%3a%2f%2fdlm.rocks%2fposts%2fmembership_inference_attack_01%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Membership Inference Attack: Primer & Case Study on telegram" href="https://telegram.me/share/url?text=Membership%20Inference%20Attack%3a%20Primer%20%26%20Case%20Study&amp;url=https%3a%2f%2fdlm.rocks%2fposts%2fmembership_inference_attack_01%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>// Works by D.Moreno //</span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod rel="noopener noreferrer" target=_blank>Papermod</a> //
Hosted on
<a href=https://firebase.google.com/ rel="noopener noreferrer" target=_blank>Firebase</a> //</span></footer><a href=#top aria-label="Go to top" title="Scroll to top"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script>let menu=document.getElementById("menu");menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function s(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>for(var els=document.getElementsByClassName("highlight"),title,code,newNode,i=0;i<els.length;i++)title=els[i].title,code=els[i].firstElementChild.firstElementChild,title.length&&(newNode=document.createElement("div"),newNode.textContent=title,newNode.classList.add("highlight-title"),code.parentNode.insertBefore(newNode,code))</script></body></html>