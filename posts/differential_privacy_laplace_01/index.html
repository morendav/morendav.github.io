<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Differential Privacy: Primer & Laplace Case Study | home</title><meta name=keywords content="Differential Privacy,Privacy,Case Study"><meta name=description content="Introduction to Differential Privacy and its definition, the additive noise mechanisms to satisfy it, and a case study to explore the effect that epsilon and source data skew have on the noise distribution."><meta name=author content="D.Moreno"><link rel=canonical href=https://dlm.rocks/posts/differential_privacy_laplace_01/><link crossorigin=anonymous href=/assets/css/stylesheet.b183800e2cfbb62c3bce2b2ba56cdb2dd33af76c75cf4550173d5dfebd7c68a6.css integrity="sha256-sYOADiz7tiw7zisrpWzbLdM692x1z0VQFz1d/r18aKY=" rel="preload stylesheet" as=style><link rel=icon href=https://dlm.rocks/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://dlm.rocks/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://dlm.rocks/favicon-32x32.png><link rel=apple-touch-icon href=https://dlm.rocks/apple-touch-icon.png><link rel=mask-icon href=https://dlm.rocks/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-H6P6WVSTW2"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-H6P6WVSTW2",{anonymize_ip:!1})}</script><meta property="og:title" content="Differential Privacy: Primer & Laplace Case Study"><meta property="og:description" content="Introduction to Differential Privacy and its definition, the additive noise mechanisms to satisfy it, and a case study to explore the effect that epsilon and source data skew have on the noise distribution."><meta property="og:type" content="article"><meta property="og:url" content="https://dlm.rocks/posts/differential_privacy_laplace_01/"><meta property="og:image" content="https://dlm.rocks/papermod-cover.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-11-10T20:09:00-08:00"><meta property="article:modified_time" content="2022-11-17T16:21:04-08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://dlm.rocks/papermod-cover.png"><meta name=twitter:title content="Differential Privacy: Primer & Laplace Case Study"><meta name=twitter:description content="Introduction to Differential Privacy and its definition, the additive noise mechanisms to satisfy it, and a case study to explore the effect that epsilon and source data skew have on the noise distribution."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://dlm.rocks/posts/"},{"@type":"ListItem","position":2,"name":"Differential Privacy: Primer \u0026 Laplace Case Study","item":"https://dlm.rocks/posts/differential_privacy_laplace_01/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Differential Privacy: Primer \u0026 Laplace Case Study","name":"Differential Privacy: Primer \u0026 Laplace Case Study","description":"Introduction to Differential Privacy and its definition, the additive noise mechanisms to satisfy it, and a case study to explore the effect that epsilon and source data skew have on the noise distribution.","keywords":["Differential Privacy","Privacy","Case Study"],"articleBody":"This post serves as a primer to Differential Privacy: presenting an intuitive foundation for the definition, proceeding to some math, and finally presenting a case study to demonstrate some key concepts.\nüìå TL;DR Differential Privacy is a class of algorithms and a formal mathematical definition that, if met, provides an upper bound to the potential privacy loss for a query response (data release). The formulation stems from the intuition that we can reason about (and quantify) the privacy of some algorithm that shrouds in uncertainty whether or not a user‚Äôs data was in the dataset that informed the result. Differential Privacy is a definition There isn‚Äôt a single solution to meet the definition, instead there are multiple mechanisms that can be used Differentially Private algorithms allow us to quantify the privacy loss For any single query we are able to quantify the upper bound of the amount of secret information a response may reveal DP is composable, so for a series of queries we can quantify the total privacy loss / Proof / Epsilon is our measure of privacy loss For series of queries run over the same dataset- we can set a global epsilon value and budget across all the queries Lower epsilon = lower accuracy, higher noise, and greater privacy / Proof / and / Case Study/ The Theory: What is Differential Privacy The foundation of Differential Privacy (DP) is the notion that an individual‚Äôs privacy is protected in a dataset if the individual‚Äôs contribution is simply excluded from the set of records, assuming data is independent.\nConsider an algorithm that takes as input a database of user records, and outputs the average of all records. If this algorithm excluded from its calculation the contribution from a target user, or if the database did not include any contribution from that target user, then as long as records in the database were independent it follows that within the scope of what can be revealed by that algorithm the target‚Äôs data stays a secret. Specifically, DP concerns itself with shrouding in uncertainty a target‚Äôs participation in the dataset that produced the output that was released.\nDP algorithms are a class of algorithms that are derived from this intuition, and that carry a mathematical guarantee for the upper-bound privacy loss of each query or data release. This mathematical formulation was the culmination of decades of privacy research, which demonstrated that prior privacy preserving standards were leaky: with enough time and repeated releases it is infeasible to make any lasting guarantee about privacy. 1 This insight motivated the development of a new standard which stemmed from the missing member in database intuition and was presented in the seminal work by C. Dwork et al. ‚ÄúCalibrating Noise to Sensitivity in Private Data Analysis‚Äù. 2 In their work they introduced the mathematics to describe a DP algorithm, as well as a generalized approach to implementation which, in its original or derivative forms, has since become recognized as the gold standard for privacy preserving data transformations.\nFrom an adversarial point of view, DP roughly means that for a given query output we would be uncertain (quantifiably so) that the output was derived from a database that did or did not contain a target user‚Äôs data. Paying homage to the cryptographic roots of privacy research and differential privacy, this is analogous to semantic security in a cryptosystem.\nWhy was DP groundbreaking? DP departed from previous attempts at generalized privacy preserving algorithms and standards notably with two characteristics:\nA quantification of the upper-bound privacy loss that might result for a data release Formalization of the privacy loss for repeated data releases from the same source data The second of these observations is a result of DP being composable, this property allows data curators to set a global maximum privacy budget which is divided among the applications of the confidential data - the proof and practical interpretation on this later in this post.\nTogether, these new characteristics of the data processing mean that a system that adheres to the strict mathematical definition that is DP allows us to reason about and quantify the privacy risk against privacy attacks today as well as into the future. 3\nThe Math: Derivation of Differential Privacy An algorithm \\(\\mathbb{A}\\) is said to satisfy epsilon Differential Privacy (ŒµDP) 2 if for all subsets \\(S\\) in the range \\(\\mathbb{A}\\) , for the databases \\( D_1 \\) and \\( D_2 \\) where [ \\( D_1, D_2\\) ] differ in at most the contribution of a single member (alternatively, differ in at most an arbitrary perturbation of a single record): $$ \\tag{1} Pr \\Big(\\mathbb{A}(D_1) \\in S \\Big) \\leq\t\\ e ^ \\epsilon \\cdotp\tPr \\Big(\\mathbb{A}(D_2) \\in S \\Big) $$\nWhere (\\(\\epsilon\\) \u003e 0). Epsilon (Œµ) is called the privacy loss or privacy budget. Rearranging this gives some insight to what epsilon means: $$ \\tag{2} \\frac {Pr\\Big(\\mathbb{A}(D_1) \\in S \\Big)} {Pr \\Big(\\mathbb{A}(D_2) \\in S \\Big)}\\ \\leq e ^ \\epsilon $$\nThis rearranged form reveals that (\\(\\epsilon\\)) is at most a function of the ratio of probabilities that an observation resulted from either \\( D_1 \\) or \\( D_2 \\). The expression in Equation 01 is absolute for the entire space of the algorithm‚Äôs output, and it is this strict definition that affords the guarantee that an adversary is limited to the upper bound over the whole range of \\(\\mathbb{A}\\). In practice, this strict definition may be relaxed with the addition of a privacy error term Œ¥ 4 :\n$$ \\tag{3} Pr \\Big(\\mathbb{A}(D_1) \\in S \\Big) \\leq\t\\ e ^ \\epsilon \\cdotp\tPr \\Big(\\mathbb{A}(D_2) \\in S \\Big) + \\delta $$\nThis error term can be roughly interpreted as the likelihood that the strict definition doesn‚Äôt hold. For brevity‚Äôs sake, I will introduce this here and leave as a teaser the fact that the error term can exist and that it exists to balance engineering practicality with the otherwise strict DP definition (besides‚Ä¶ I need to save some content for future idea sharing posts! üòâ)\nA quick proof‚Ä¶ Quantifying Privacy Loss for Repeated Queries In order to reason about the upper bound for repeated queries we need to prove that ŒµDP is sequentially composable:\nLet \\(\\mathbb{A}_1\\) and \\(\\mathbb{A}_2\\) be algorithms that satisfy Equation 01 and are thus ŒµDP with \\(\\epsilon_1\\) and \\(\\epsilon_2\\) privacy loss parameters respectively. Let databases \\( D_1 \\) or \\( D_2 \\) differ by at most 1 subject's contribution, and whose output are spaces \\(\\mathcal{R_1} \\) and \\(\\mathcal{R_2}\\). So far we have: $$ \\tag{4.1} Pr \\Big(\\mathbb{A_1}(D_1) \\in \\mathcal{R_1} \\Big) \\leq\t\\ e ^ {\\epsilon_1} \\cdotp\tPr \\Big(\\mathbb{A_1}(D_2) \\in \\mathcal{R_1} \\Big) $$ $$ \\tag{4.2} Pr \\Big(\\mathbb{A_2}(D_1) \\in \\mathcal{R_2} \\Big) \\leq\t\\ e ^ {\\epsilon_2} \\cdotp\tPr \\Big(\\mathbb{A_2}(D_2) \\in \\mathcal{R_2} \\Big) $$\nIf a third algorithm, \\(\\mathbb{C}\\), is a combination of both ( \\(\\mathbb{A}_1, \\mathbb{A}_2\\) ) such that \\(\\mathbb{C} \\to \\mathcal{R_1} √ó \\mathcal{R_2} \\). Then, for some algorithm output ( \\(\\mathbb{A}_1 \\to r_1\\) ) and (\\(\\mathbb{A}_2 \\to r_2\\) ) where ( \\(r_1, r_2 ) \\in \\mathcal{R_1} √ó \\mathcal{R_2} \\) then we can state for \\(\\mathbb{C}\\): $$ \\tag{4.3} \\frac {Pr \\Big(\\mathbb{C}(D_1) = (r_1, r_2 ) \\Big)} {Pr \\Big(\\mathbb{C}(D_2) = (r_1, r_2 ) \\Big)} = \\frac {Pr \\Big(\\mathbb{A_1}(D_1) = r_1 \\Big)} {Pr \\Big(\\mathbb{A_1}(D_2) = r_1 \\Big)} \\frac {Pr \\Big(\\mathbb{A_2}(D_1) = r_2 \\Big)} {Pr \\Big(\\mathbb{A_2}(D_2) = r_2 \\Big)} $$\nBy a matter of substitution using Equation 2, we have:\n$$ \\tag{4.4} \\frac {Pr \\Big(\\mathbb{C}(D_1) = (r_1, r_2 ) \\Big)} {Pr \\Big(\\mathbb{C}(D_2) = (r_1, r_2 ) \\Big)} \\leq e ^ {\\epsilon_1} e ^ {\\epsilon_2} $$\n$$ \\tag{4.4} \\frac {Pr \\Big(\\mathbb{C}(D_1) = (r_1, r_2 ) \\Big)} {Pr \\Big(\\mathbb{C}(D_2) = (r_1, r_2 ) \\Big)} \\leq e ^ {\\epsilon_1 + \\epsilon_2} $$\nThis can be generalized for any algorithm that is the composite of N independent differentially private algorithms:\n$$ \\tag{4.5} \\frac {Pr \\Big(\\mathbb{C}(D_1) = (r_1, r_2, ‚Ä¶ r_n) ) \\Big)} {Pr \\Big(\\mathbb{C}(D_2) = (r_1, r_2, ‚Ä¶ r_n) \\Big)} \\leq e ^ {\\epsilon_1 + \\epsilon_2 + ‚Ä¶ + \\epsilon_n} $$\nThe sum of all privacy losses is the global privacy loss, which can be divided between the N algorithms as needed. The composability of DP tells us that theoretically we can assert that at most privacy erodes linearly with respect to n-epsilons, though in practice it is sublinear.\nAnd a bit more math‚Ä¶ Poking at Œµ Some reasoning about how to set ( \\(\\epsilon \\) ), and what it means for the analysis is readily developed by poking at Equation 02 with limits: Let \\(\\epsilon \\rightarrow 0\\) $$ \\tag{2} \\Bigg( \\frac {Pr\\Big(\\mathbb{A}(D_1) \\in S \\Big)} {Pr \\Big(\\mathbb{A}(D_2) \\in S \\Big)} \\leq { e ^ \\epsilon } \\Bigg) {\\Bigg\\vert _{\\epsilon \\rightarrow 0}} $$\n$$ \\tag{2.1} \\frac {Pr\\Big(\\mathbb{A}(D_1) \\in S \\Big)} {Pr \\Big(\\mathbb{A}(D_2) \\in S \\Big)} \\leq 1 $$\nFor the case where \\(\\epsilon \\rightarrow 0\\), for some given output in range \\(S\\) we are at most equally likely to deduce that the output has come from \\(D_1\\) or \\(D_2\\). This means that we have maximized the uncertainty as to which dataset informed the output. Within the context of the opening intuition: maximum uncertainty is maximum privacy. In Practice DP‚Äôs definition is really a conceptual property that describes a data transformation. There are a few potential transformations that satisfy this property, some popular mechanisms are additive noise mechanisms which sample noise from a distribution and inject that noise into the query response. Some popular additive noise mechanisms are:\nLaplace Mechanism This mechanism is the canonical DP mechanism that satisfies Œµ-DP (Equation 01), and is otherwise known as ‚Äòpure differential privacy‚Äô. This mechanism samples noise from a zero-centered Laplace distribution and injects this noise into the data release. 2\nGaussian Mechanism The Gaussian mechanism is very similar to the Laplace mechanism, except it uses a zero-centered, variance bound Guassian distribution and does not satisfy Œµ-DP, instead it satisfies (Œµ, Œ¥) Differential Privacy (Equation 03). In addition to restrictions on the Noise-Distribution‚Äôs variance, this additive noise mechanism only holds true to (Œµ, Œ¥) DP when œµ \u003c 1.\nüî® Implementation tips Differential Privacy is a powerful tool, while wielding it watch for:\nDimensions in the source data that are correlated: this leads to leaky privacy guarantees Extreme outliers in the data: users in the long tails of data are more identifiable, require more noise to mask. Consider binning data into ranges to address this. This is also the subject of the case study DP with Privacy Budget Accounting Composition has been demonstrated for ŒµDP algorithms, which allows us to define a global limit on the maximum privacy loss that we will allow for a given dataset. When this global privacy loss is set, multiple queries are each assigned some portion of the whole budget. In other words, repeated queries deplete the global privacy budget.\nReferring back to Equation 02 - note that for lower Œµ values we expect more uncertainty in which of the datasets the algorithm‚Äôs output came from D1 or D2. As a generalization we have:\nLower Œµ value:\nHigher the noise we expect in the query response Lower accuracy in the query response as a result of the noise Better privacy story, but at the expense of the data utility üîß Œµ Setting Tips Fixing the global privacy budget means that care is needed when accounting for where to allocate higher accuracy queries, or where a sacrifice on accuracy may be made to stay within the budget.\nIn addition to careful accounting, cached answers can extend the budget. During the fulfillment of the query-response protocol: cache answers to queries, and retrieve cached answers when the same query is issued multiple times to the database. This type of response-replay won‚Äôt deplete the privacy budget, and ensures no additional information is learned about the data‚Äôs subjects for repeated queries.\nCase Study The following case study is hosted on GitHub, which includes a Creative Commons licensed dataset (thanks Kaggle!). In this case study we will be exploring the implementation of OpenMined PyDP which is a python wrapper that makes use of Google‚Äôs open source C++ Differential Privacy library. This Python library uses the Laplace mechanism of additive noise to satisfy ŒµDP.\nObjective: In this case study we will see the effect that:\nDifferent Œµ values have on the differential privacy noise Long tail skew in the source data have on differential privacy noise The Data For the fulfillment of this case study the data will first need to be:\nSanitized (e.g. correct string entries in numeric columns, etc) Columns dropped (the data cube diced) In addition to this clean up, we can drop all but the last month‚Äôs data for simplicity‚Äôs sake (data cube sliced). For the remainder of the case study we will be using the ‚ÄòAnnual_Income‚Äò dimension only.\nAnnual Income plotted in linear and log scale This is a terrific dimension to explore: it exhibits a rich (pun intended üí∞), long tailed right skew. Note that the tail is so long that it is impractical to plot annual income in linear scale (left), in log-scale we get a greater sense of where most users are and where only a few, very wealthy users are.\nThe Private Queries The Git Python project includes methods for private queries:\nRepeated_average Repeated_sum Repated_max Which take as input:\niterations (integer) number of times the private query will be repeated privacy_budget (float) epsilon value, does not change between iterations list (array) dataframe dimension to list, that is used as source data for the private query These methods do not use any privacy loss budget accounting, as a result over N-Iterations we expect a nice Laplace distribution in the observed query responses that should center on the true value.\nNoise \u0026 Œµ Running the repeated average method over 100 iterations, with Œµ = [1,100] returns the following histograms of observed query respones:\nRepeated Private Average Queries, 100 iterations with Epsilon = 100 Repeated Private Average Queries, 100 iterations with Epsilon = 1 As expected, a higher Œµ value yields query responses with lower noise injected - demonstrated by a distribution with a tighter spread, both distributions are centered on the true value which is represented by a vertical line.\nNoise \u0026 Long-tailed data We can imagine that blending in at a crowded event is much easier than trying to blend in in an otherwise empty room; so too there is a certain degree of privacy protection among a concentrated band of data rather than in a long tail. In other words, being a member of the 107 annual income club makes it difficult to apply enough noise to mask your contribution in the annual income dataset when running a private average query.\nRunning repeated private queries with iterations = 1000 and Œµ = 0.85 for the raw dataset (right skew, without any skew correction) and again for a skew-corrected dataset (still right skew, but cutting off anyone that isn‚Äôt below the 95% percentile).\nFor skew correction, we want to keep the count of unique users (in the dataset, count of rows) the same between datasets to make an apple to apple comparison. To address this, the skew-correction method takes any entry above 95 percentile, and replaces their contribution to annual income to be something within the sub-95 percentile band (using a random number generator within a bound).\nNormalized noise observations for repeated private queries, 1000 iterations, epsilon = 0.85 These histograms are normalized to their respective distributions‚Äô average salary. Eliminating the long tail results in over a degree of magnitude change for the normalized noise distributions. üëÄ\nIrit Dinur and Kobbi Nissim, Revealing information while preserving privacy¬†‚Ü©Ô∏é\nCynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith, Calibrating noise to sensitivity in private data analysis¬†‚Ü©Ô∏é¬†‚Ü©Ô∏é¬†‚Ü©Ô∏é\nKobbi Nissim , Thomas Steinke , Alexandra Wood, Micah Altman, Aaron Bembenek, Mark Bun, Marco Gaboardi, David R. O‚ÄôBrien, and Salil Vadhan, Differential Privacy: A Primer for a Non-technical Audience¬†‚Ü©Ô∏é\nCynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor, Our data, Ourselves: Privacy via Distributions Noise Generation¬†‚Ü©Ô∏é\n","wordCount":"2639","inLanguage":"en","datePublished":"2022-11-10T20:09:00-08:00","dateModified":"2022-11-17T16:21:04-08:00","author":{"@type":"Person","name":"D.Moreno"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://dlm.rocks/posts/differential_privacy_laplace_01/"},"publisher":{"@type":"Organization","name":"home","logo":{"@type":"ImageObject","url":"https://dlm.rocks/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://dlm.rocks/ accesskey=h title="home (Alt + H)">home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://dlm.rocks/archives title=archive><span>archive</span></a></li><li><a href=https://dlm.rocks/tags/ title=tags><span>tags</span></a></li><li><a href=https://dlm.rocks/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://dlm.rocks/about/ title=about><span>about</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://dlm.rocks/>Home</a>&nbsp;¬ª&nbsp;<a href=https://dlm.rocks/posts/>Posts</a></div><h1 class=post-title>Differential Privacy: Primer & Laplace Case Study</h1><div class=post-description>Introduction to Differential Privacy and its definition, the additive noise mechanisms to satisfy it, and a case study to explore the effect that epsilon and source data skew have on the noise distribution.</div><div class=post-meta><span title='2022-11-10 20:09:00 -0800 -0800'>November 10, 2022</span>&nbsp;¬∑&nbsp;13 min&nbsp;¬∑&nbsp;D.Moreno</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#-tldr aria-label="üìå TL;DR">üìå TL;DR</a></li><li><a href=#the-theory-what-is-differential-privacy aria-label="The Theory: What is Differential Privacy">The Theory: What is Differential Privacy</a><ul><li><a href=#why-was-dp-groundbreaking aria-label="Why was DP groundbreaking?">Why was DP groundbreaking?</a></li><li><a href=#the-math-derivation-of-differential-privacy aria-label="The Math: Derivation of Differential Privacy">The Math: Derivation of Differential Privacy</a><ul><li><a href=#a-quick-proof-quantifying-privacy-loss-for-repeated-queries aria-label="A quick proof‚Ä¶ Quantifying Privacy Loss for Repeated Queries">A quick proof‚Ä¶ Quantifying Privacy Loss for Repeated Queries</a></li><li><a href=#and-a-bit-more-math-poking-at-epsilon aria-label="And a bit more math‚Ä¶ Poking at Œµ">And a bit more math‚Ä¶ Poking at Œµ</a></li></ul></li></ul></li><li><a href=#in-practice aria-label="In Practice">In Practice</a><ul><ul><li><a href=#laplace-mechanism aria-label="Laplace Mechanism">Laplace Mechanism</a></li><li><a href=#gaussian-mechanism aria-label="Gaussian Mechanism">Gaussian Mechanism</a></li><li><a href=#-implementation-tips aria-label="üî® Implementation tips">üî® Implementation tips</a></li></ul><li><a href=#dp-with-privacy-budget-accounting aria-label="DP with Privacy Budget Accounting">DP with Privacy Budget Accounting</a><ul><li><a href=#-epsilon-setting-tips aria-label="üîß Œµ Setting Tips">üîß Œµ Setting Tips</a></li></ul></li></ul></li><li><a href=#case-study aria-label="Case Study">Case Study</a><ul><li><a href=#the-data aria-label="The Data">The Data</a></li><li><a href=#the-private-queries aria-label="The Private Queries">The Private Queries</a></li><li><a href=#case_study_epsilon aria-label="Noise &amp;amp; Œµ ">Noise & Œµ</a></li><li><a href=#case_study_long_tail aria-label="Noise &amp;amp; Long-tailed data">Noise & Long-tailed data</a></li></ul></li></ul></div></details></div><div class=post-content><p>This post serves as a primer to Differential Privacy: presenting an intuitive foundation for the definition, proceeding to some math, and finally presenting a case study to demonstrate some key concepts.</p><h1 id=-tldr>üìå TL;DR<a hidden class=anchor aria-hidden=true href=#-tldr>#</a></h1><ul><li>Differential Privacy is a <em><strong>class of algorithms and a formal mathematical definition</strong></em> that, if met, provides an upper bound to the potential privacy loss for a query response (data release).</li><li>The formulation stems from the intuition that we can reason about (and quantify) the privacy of some algorithm that shrouds in <em><strong>uncertainty whether or not a user&rsquo;s data was in the dataset</strong></em> that informed the result.</li></ul><table><tr><td><strong>Differential Privacy is a definition</strong></td><td><ul><li>There isn‚Äôt a single solution to meet the definition, instead there are multiple mechanisms that can be used</li></ul></td></tr><tr><td><strong>Differentially Private algorithms allow us to quantify the privacy loss</strong></td><td><ul><li>For any single query we are able to quantify the upper bound of the amount of secret information a response may reveal<li>DP is composable, so for a series of queries we can quantify the total privacy loss <a href=/posts/differential_privacy_laplace_01/#proof_composition>/ Proof /</a></li></ul></td></tr><tr><td><strong>Epsilon is our measure of privacy loss</strong></td><td><ul><li>For series of queries run over the same dataset- we can set a global epsilon value and budget across all the queries<li>Lower epsilon = lower accuracy, higher noise, and greater privacy <a href=/posts/differential_privacy_laplace_01/#proof_epsilon>/ Proof /</a> and <a href=/posts/differential_privacy_laplace_01/#case_study_epsilon>/ Case Study/</a></li></ul></td></tr></table><hr><h1 id=the-theory-what-is-differential-privacy>The Theory: What is Differential Privacy<a hidden class=anchor aria-hidden=true href=#the-theory-what-is-differential-privacy>#</a></h1><p>The foundation of Differential Privacy (DP) is the notion that an individual‚Äôs privacy is protected in a dataset if the individual‚Äôs contribution is simply excluded from the set of records, assuming data is independent.</p><p>Consider an algorithm that takes as input a database of user records, and outputs the average of all records. If this algorithm excluded from its calculation the contribution from a target user, or if the database did not include any contribution from that target user, then as long as records in the database were independent it follows that within the scope of what can be revealed by that algorithm the target‚Äôs data stays a secret. Specifically, DP concerns itself with shrouding in uncertainty a target‚Äôs participation in the dataset that produced the output that was released.</p><p>DP algorithms are a class of algorithms that are derived from this intuition, and that carry a mathematical guarantee for the upper-bound privacy loss of each query or data release. This mathematical formulation was the culmination of decades of privacy research, which demonstrated that prior privacy preserving standards were leaky: with enough time and repeated releases it is infeasible to make any lasting guarantee about privacy. <sub><cite><sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></cite></sub> This insight motivated the development of a new standard which stemmed from the <em>missing member in database</em> intuition and was presented in the seminal work by C. Dwork et al. ‚ÄúCalibrating Noise to Sensitivity in Private Data Analysis‚Äù. <sub><cite><sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></cite></sub> In their work they introduced the mathematics to describe a DP algorithm, as well as a generalized approach to implementation which, in its original or derivative forms, has since become recognized as the gold standard for privacy preserving data transformations.</p><p>From an adversarial point of view, DP roughly means that for a given query output we would be uncertain (quantifiably so) that the output was derived from a database that did or did not contain a target user‚Äôs data. Paying homage to the cryptographic roots of privacy research and differential privacy, this is analogous to <em>semantic security</em> in a cryptosystem.</p><h2 id=why-was-dp-groundbreaking>Why was DP groundbreaking?<a hidden class=anchor aria-hidden=true href=#why-was-dp-groundbreaking>#</a></h2><p>DP departed from previous attempts at generalized privacy preserving algorithms and standards notably with two characteristics:</p><ol><li>A quantification of the upper-bound privacy loss that might result for a data release</li><li>Formalization of the privacy loss for repeated data releases from the same source data</li></ol><p>The second of these observations is a result of DP being composable, this property allows data curators to set a global maximum privacy budget which is divided among the applications of the confidential data - the proof and practical interpretation on this later in this post.</p><p>Together, these new characteristics of the data processing mean that a system that adheres to the strict mathematical definition that is DP allows us to reason about and quantify the privacy risk against privacy attacks today as well as into the future. <sub><cite><sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup></cite></sub></p><h2 id=the-math-derivation-of-differential-privacy>The Math: Derivation of Differential Privacy<a hidden class=anchor aria-hidden=true href=#the-math-derivation-of-differential-privacy>#</a></h2><p>An algorithm \(\mathbb{A}\)
is said to satisfy epsilon Differential Privacy (ŒµDP) <cite><sub><sup id=fnref1:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></sub></cite>
if for all subsets \(S\) in the range \(\mathbb{A}\) , for the databases \( D_1 \) and \( D_2 \) where [ \( D_1, D_2\) ] differ in at most the contribution of a single member (alternatively, differ in at most an arbitrary perturbation of a single record):</p><p>$$
\tag{1} Pr \Big(\mathbb{A}(D_1) \in S \Big) \leq \
e ^ \epsilon \cdotp Pr \Big(\mathbb{A}(D_2) \in S \Big)
$$</p><p>Where (\(\epsilon\) > 0). Epsilon (&#949;) is called the privacy loss or privacy budget. Rearranging this gives some insight to what epsilon means:</p><h4 id=proof_epsilon><a hidden class=anchor aria-hidden=true href=#proof_epsilon>#</a></h4><p>$$
\tag{2} \frac {Pr\Big(\mathbb{A}(D_1) \in S \Big)} {Pr \Big(\mathbb{A}(D_2) \in S \Big)}\
\leq e ^ \epsilon
$$</p><p><p>This rearranged form reveals that (\(\epsilon\)) is at most a function of the ratio of probabilities that an observation resulted from either \( D_1 \) or \( D_2 \).</p>The expression in Equation 01 is absolute for the entire space of the algorithm‚Äôs output, and it is this strict definition that affords the guarantee that an adversary is limited to the upper bound over the whole range of \(\mathbb{A}\). In practice, this strict definition may be relaxed with the addition of a privacy error term
Œ¥ <sub><cite><sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup></cite></sub> :</p><p>$$
\tag{3} Pr \Big(\mathbb{A}(D_1) \in S \Big) \leq \
e ^ \epsilon \cdotp Pr \Big(\mathbb{A}(D_2) \in S \Big) + \delta
$$</p><p>This error term can be roughly interpreted as the likelihood that the strict definition doesn‚Äôt hold. For brevity‚Äôs sake, I will introduce this here and leave as a teaser the fact that the error term can exist and that it exists to balance engineering practicality with the otherwise strict DP definition (besides‚Ä¶ I need to save some content for future idea sharing posts! üòâ)</p><h3 id=a-quick-proof-quantifying-privacy-loss-for-repeated-queries>A quick proof‚Ä¶ Quantifying Privacy Loss for Repeated Queries<a hidden class=anchor aria-hidden=true href=#a-quick-proof-quantifying-privacy-loss-for-repeated-queries>#</a></h3><p>In order to reason about the upper bound for repeated queries we need to prove that ŒµDP is sequentially composable:</p><p>Let \(\mathbb{A}_1\) and \(\mathbb{A}_2\) be algorithms that satisfy Equation 01 and are thus &#949;DP with \(\epsilon_1\) and \(\epsilon_2\) privacy loss parameters respectively. Let databases \( D_1 \) or \( D_2 \) differ by at most 1 subject's contribution, and whose output are spaces \(\mathcal{R_1} \) and \(\mathcal{R_2}\). So far we have:</p><h4 id=proof_composition><a hidden class=anchor aria-hidden=true href=#proof_composition>#</a></h4><p>$$
\tag{4.1} Pr \Big(\mathbb{A_1}(D_1) \in \mathcal{R_1} \Big) \leq \
e ^ {\epsilon_1} \cdotp Pr \Big(\mathbb{A_1}(D_2) \in \mathcal{R_1} \Big)
$$
$$
\tag{4.2} Pr \Big(\mathbb{A_2}(D_1) \in \mathcal{R_2} \Big) \leq \
e ^ {\epsilon_2} \cdotp Pr \Big(\mathbb{A_2}(D_2) \in \mathcal{R_2} \Big)
$$</p><p>If a third algorithm, \(\mathbb{C}\), is a combination of both ( \(\mathbb{A}_1, \mathbb{A}_2\) ) such that \(\mathbb{C} \to \mathcal{R_1} √ó \mathcal{R_2} \). Then, for some algorithm output ( \(\mathbb{A}_1 \to r_1\) ) and (\(\mathbb{A}_2 \to r_2\) ) where ( \(r_1, r_2 ) \in \mathcal{R_1} √ó \mathcal{R_2} \) then we can state for \(\mathbb{C}\):</p><p>$$
\tag{4.3}
\frac
{Pr \Big(\mathbb{C}(D_1) = (r_1, r_2 ) \Big)}
{Pr \Big(\mathbb{C}(D_2) = (r_1, r_2 ) \Big)}
=
\frac
{Pr \Big(\mathbb{A_1}(D_1) = r_1 \Big)}
{Pr \Big(\mathbb{A_1}(D_2) = r_1 \Big)}
\frac
{Pr \Big(\mathbb{A_2}(D_1) = r_2 \Big)}
{Pr \Big(\mathbb{A_2}(D_2) = r_2 \Big)}
$$</p><p>By a matter of substitution using Equation 2, we have:</p><p>$$
\tag{4.4}
\frac
{Pr \Big(\mathbb{C}(D_1) = (r_1, r_2 ) \Big)}
{Pr \Big(\mathbb{C}(D_2) = (r_1, r_2 ) \Big)}
\leq
e ^ {\epsilon_1}
e ^ {\epsilon_2}
$$</p><p>$$
\tag{4.4}
\frac
{Pr \Big(\mathbb{C}(D_1) = (r_1, r_2 ) \Big)}
{Pr \Big(\mathbb{C}(D_2) = (r_1, r_2 ) \Big)}
\leq
e ^ {\epsilon_1 + \epsilon_2}
$$</p><p>This can be generalized for any algorithm that is the composite of N independent differentially private algorithms:</p><p>$$
\tag{4.5}
\frac
{Pr \Big(\mathbb{C}(D_1) = (r_1, r_2, &mldr; r_n) ) \Big)}
{Pr \Big(\mathbb{C}(D_2) = (r_1, r_2, &mldr; r_n) \Big)}
\leq
e ^ {\epsilon_1 + \epsilon_2 + &mldr; + \epsilon_n}
$$</p><p>The sum of all privacy losses is the global privacy loss, which can be divided between the N algorithms as needed. The composability of DP tells us that theoretically we can assert that at most privacy erodes linearly with respect to n-epsilons, though in practice it is sublinear.</p><h3 id=and-a-bit-more-math-poking-at-epsilon>And a bit more math‚Ä¶ Poking at Œµ<a hidden class=anchor aria-hidden=true href=#and-a-bit-more-math-poking-at-epsilon>#</a></h3><p>Some reasoning about how to set ( \(\epsilon \) ), and what it means for the analysis is readily developed by poking at Equation 02 with limits:</p><p>Let \(\epsilon \rightarrow 0\)</p><p>$$
\tag{2}
\Bigg(
\frac
{Pr\Big(\mathbb{A}(D_1) \in S \Big)}
{Pr \Big(\mathbb{A}(D_2) \in S \Big)}
\leq
{ e ^ \epsilon }
\Bigg)
{\Bigg\vert _{\epsilon \rightarrow 0}}
$$</p><p>$$
\tag{2.1}
\frac
{Pr\Big(\mathbb{A}(D_1) \in S \Big)}
{Pr \Big(\mathbb{A}(D_2) \in S \Big)}
\leq
1
$$</p><p>For the case where \(\epsilon \rightarrow 0\), for some given output in range \(S\) we are at most equally likely to deduce that the output has come from \(D_1\) or \(D_2\). This means that we have maximized the uncertainty as to which dataset informed the output. Within the context of the opening intuition: maximum uncertainty is maximum privacy.</p><hr><h1 id=in-practice>In Practice<a hidden class=anchor aria-hidden=true href=#in-practice>#</a></h1><p>DP‚Äôs definition is really a conceptual property that describes a data transformation. There are a few potential transformations that satisfy this property, some popular mechanisms are <em>additive noise mechanisms</em> which sample noise from a distribution and inject that noise into the query response. Some popular additive noise mechanisms are:</p><h3 id=laplace-mechanism>Laplace Mechanism<a hidden class=anchor aria-hidden=true href=#laplace-mechanism>#</a></h3><p>This mechanism is the canonical DP mechanism that satisfies Œµ-DP (Equation 01), and is otherwise known as ‚Äòpure differential privacy‚Äô. This mechanism samples noise from a zero-centered <a href=https://en.wikipedia.org/wiki/Laplace_distribution>Laplace distribution</a> and injects this noise into the data release. <sub><cite><sup id=fnref2:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></cite></sub></p><h3 id=gaussian-mechanism>Gaussian Mechanism<a hidden class=anchor aria-hidden=true href=#gaussian-mechanism>#</a></h3><p>The Gaussian mechanism is very similar to the Laplace mechanism, except it uses a zero-centered, variance bound <a href=https://en.wikipedia.org/wiki/Normal_distribution>Guassian distribution</a> and does not satisfy Œµ-DP, instead it satisfies (Œµ, Œ¥) Differential Privacy (Equation 03). In addition to restrictions on the Noise-Distribution‚Äôs variance, this additive noise mechanism only holds true to (Œµ, Œ¥) DP when œµ &lt; 1.</p><h3 id=-implementation-tips>üî® Implementation tips<a hidden class=anchor aria-hidden=true href=#-implementation-tips>#</a></h3><p>Differential Privacy is a powerful tool, while wielding it watch for:</p><ul><li><em><strong>Dimensions in the source data that are correlated</strong></em>: this leads to leaky privacy guarantees</li><li><em><strong>Extreme outliers in the data</strong></em>: users in the long tails of data are more identifiable, require more noise to mask. Consider binning data into ranges to address this. This is also the <a href=/posts/differential_privacy_laplace_01/#case_study_long_tail>subject of the case study</a></li></ul><h2 id=dp-with-privacy-budget-accounting>DP with Privacy Budget Accounting<a hidden class=anchor aria-hidden=true href=#dp-with-privacy-budget-accounting>#</a></h2><p>Composition has been demonstrated for ŒµDP algorithms, which allows us to define a global limit on the maximum privacy loss that we will allow for a given dataset. When this global privacy loss is set, multiple queries are each assigned some portion of the whole budget. In other words, repeated queries <strong><em>deplete the global privacy budget</em></strong>.</p><p>Referring back to Equation 02 - note that for lower Œµ values we expect more uncertainty in which of the datasets the algorithm‚Äôs output came from D<sub>1</sub> or D<sub>2</sub>. As a generalization we have:</p><p><strong>Lower Œµ value:</strong></p><ul><li>Higher the noise we expect in the query response</li><li>Lower accuracy in the query response as a result of the noise</li><li>Better privacy story, but at the expense of the data utility</li></ul><h3 id=-epsilon-setting-tips>üîß Œµ Setting Tips<a hidden class=anchor aria-hidden=true href=#-epsilon-setting-tips>#</a></h3><p>Fixing the global privacy budget means that <em><strong>care is needed when accounting for where to allocate higher accuracy queries</strong></em>, or where a sacrifice on accuracy may be made to stay within the budget.</p><p>In addition to careful accounting, <em><strong>cached answers can extend the budget</strong></em>. During the fulfillment of the query-response protocol: cache answers to queries, and retrieve cached answers when the same query is issued multiple times to the database. This type of response-replay won‚Äôt deplete the privacy budget, and ensures no additional information is learned about the data‚Äôs subjects for repeated queries.</p><hr><h1 id=case-study>Case Study<a hidden class=anchor aria-hidden=true href=#case-study>#</a></h1><p>The following case study is <a href=https://github.com/morendav/pydp_laplace>hosted on GitHub</a>, which includes a Creative Commons licensed <a href=https://www.kaggle.com/datasets/parisrohan/credit-score-classification>dataset</a> (thanks Kaggle!).
In this case study we will be exploring the implementation of <a href=https://github.com/OpenMined/PyDP>OpenMined PyDP</a> which is a python wrapper that makes use of Google‚Äôs open source <a href=https://github.com/google/differential-privacy>C++ Differential Privacy library</a>. This Python library uses the Laplace mechanism of additive noise to satisfy ŒµDP.</p><p><strong>Objective</strong>: In this case study we will see the effect that:</p><ol><li>Different Œµ values have on the differential privacy noise</li><li>Long tail skew in the source data have on differential privacy noise</li></ol><h2 id=the-data>The Data<a hidden class=anchor aria-hidden=true href=#the-data>#</a></h2><p>For the fulfillment of this case study the data will first need to be:</p><ul><li>Sanitized (e.g. correct string entries in numeric columns, etc)</li><li>Columns dropped (the data cube diced)</li></ul><p>In addition to this clean up, we can drop all but the last month‚Äôs data for simplicity&rsquo;s sake (data cube sliced). For the remainder of the case study we will be using the ‚ÄòAnnual_Income‚Äò dimension only.</p><figure class=align-center><a href=https://github.com/morendav/pydp_laplace/blob/main/1000%20iterations%2C%20epsilon%200.85/annual_income_distribution_plots.png target=_blank><img loading=lazy src=https://raw.githubusercontent.com/morendav/pydp_laplace/main/1000%20iterations%2C%20epsilon%200.85/annual_income_distribution_plots.png></a><figcaption>Annual Income plotted in linear and log scale</figcaption></figure><p>This is a terrific dimension to explore: it exhibits a rich (pun intended üí∞), long tailed right skew. Note that the tail is so long that it is impractical to plot annual income in linear scale (left), in log-scale we get a greater sense of where most users are and where only a few, very wealthy users are.</p><h2 id=the-private-queries>The Private Queries<a hidden class=anchor aria-hidden=true href=#the-private-queries>#</a></h2><p>The Git <a href=https://github.com/morendav/pydp_laplace/blob/main/laplace.py>Python project</a> includes methods for private queries:</p><ul><li>Repeated_average</li><li>Repeated_sum</li><li>Repated_max</li></ul><p>Which take as input:</p><ul><li>iterations (integer) number of times the private query will be repeated</li><li>privacy_budget (float) epsilon value, does not change between iterations</li><li>list (array) dataframe dimension to list, that is used as source data for the private query</li></ul><p>These methods <em><span style=text-decoration:underline>do not use any privacy loss budget accounting</span></em>, as a result over N-Iterations we expect a nice Laplace distribution in the observed query responses that should center on the true value.</p><h2 id=case_study_epsilon>Noise & Œµ <a hidden class=anchor aria-hidden=true href=#case_study_epsilon>#</a></h2><p>Running the repeated average method over 100 iterations, with Œµ = [1,100] returns the following histograms of observed query respones:</p><figure class=align-center><a href=https://github.com/morendav/pydp_laplace/blob/main/100%20iterations%2C%20epsilon%20100/repeated_average_historgram.png target=_blank><img loading=lazy src=https://raw.githubusercontent.com/morendav/pydp_laplace/main/100%20iterations%2C%20epsilon%20100/repeated_average_historgram.png></a><figcaption>Repeated Private Average Queries, 100 iterations with Epsilon = 100</figcaption></figure><figure class=align-center><a href=https://github.com/morendav/pydp_laplace/blob/main/100%20iterations%2C%20epsilon%201/repeated_average_historgram.png target=_blank><img loading=lazy src=https://raw.githubusercontent.com/morendav/pydp_laplace/main/100%20iterations%2C%20epsilon%201/repeated_average_historgram.png></a><figcaption>Repeated Private Average Queries, 100 iterations with Epsilon = 1</figcaption></figure><p>As expected, a higher Œµ value yields query responses with lower noise injected - demonstrated by a distribution with a tighter spread, both distributions are centered on the true value which is represented by a vertical line.</p><h2 id=case_study_long_tail>Noise & Long-tailed data<a hidden class=anchor aria-hidden=true href=#case_study_long_tail>#</a></h2><p>We can imagine that blending in at a crowded event is much easier than trying to blend in in an otherwise empty room; so too there is a certain degree of privacy protection among a concentrated band of data rather than in a long tail. In other words, being a member of the 10<sup>7</sup> annual income club makes it difficult to apply enough noise to mask your contribution in the annual income dataset when running a private average query.</p><p>Running repeated private queries with iterations = 1000 and Œµ = 0.85 for the raw dataset (right skew, without any skew correction) and again for a skew-corrected dataset (still right skew, but cutting off anyone that isn‚Äôt below the 95% percentile).</p><p>For skew correction, we want to keep the count of unique users (in the dataset, count of rows) the same between datasets to make an apple to apple comparison. To address this, the skew-correction method takes any entry above 95 percentile, and replaces their contribution to annual income to be something within the sub-95 percentile band (using a random number generator within a bound).</p><figure class=align-center><a href=https://github.com/morendav/pydp_laplace/blob/main/1000%20iterations%2C%20epsilon%200.85/normalized_noise_observations.png target=_blank><img loading=lazy src=https://raw.githubusercontent.com/morendav/pydp_laplace/main/1000%20iterations%2C%20epsilon%200.85/normalized_noise_observations.png></a><figcaption>Normalized noise observations for repeated private queries, 1000 iterations, epsilon = 0.85</figcaption></figure><p>These histograms are normalized to their respective distributions&rsquo; average salary. Eliminating the long tail results in over a degree of magnitude change for the normalized noise distributions. üëÄ</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Irit Dinur and Kobbi Nissim, <a href=https://crypto.stanford.edu/seclab/sem-03-04/psd.pdf>Revealing information while preserving privacy</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith, <a href=https://link.springer.com/chapter/10.1007/11681878_14>Calibrating noise to sensitivity in private data analysis</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>Kobbi Nissim , Thomas Steinke , Alexandra Wood, Micah Altman, Aaron Bembenek, Mark Bun, Marco Gaboardi, David R. O‚ÄôBrien, and Salil Vadhan, <a href=https://privacytools.seas.harvard.edu/files/privacytools/files/pedagogical-document-dp_new.pdf>Differential Privacy: A Primer for a Non-technical Audience</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor, <a href=https://www.iacr.org/archive/eurocrypt2006/40040493/40040493.pdf>Our data, Ourselves: Privacy via Distributions Noise Generation</a>&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://dlm.rocks/tags/differential-privacy/>Differential Privacy</a></li><li><a href=https://dlm.rocks/tags/privacy/>Privacy</a></li><li><a href=https://dlm.rocks/tags/case-study/>Case Study</a></li></ul><nav class=paginav><a class=prev href=https://dlm.rocks/posts/ai_art_wins_art_contest/><span class=title>¬´ Prev</span><br><span>AI in Art & Creativity</span></a>
<a class=next href=https://dlm.rocks/posts/how_this_all_started/><span class=title>Next ¬ª</span><br><span>How this all started...</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Differential Privacy: Primer & Laplace Case Study on twitter" href="https://twitter.com/intent/tweet/?text=Differential%20Privacy%3a%20Primer%20%26%20Laplace%20Case%20Study&url=https%3a%2f%2fdlm.rocks%2fposts%2fdifferential_privacy_laplace_01%2f&hashtags=DifferentialPrivacy%2cPrivacy%2cCaseStudy"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Differential Privacy: Primer & Laplace Case Study on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fdlm.rocks%2fposts%2fdifferential_privacy_laplace_01%2f&title=Differential%20Privacy%3a%20Primer%20%26%20Laplace%20Case%20Study&summary=Differential%20Privacy%3a%20Primer%20%26%20Laplace%20Case%20Study&source=https%3a%2f%2fdlm.rocks%2fposts%2fdifferential_privacy_laplace_01%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Differential Privacy: Primer & Laplace Case Study on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fdlm.rocks%2fposts%2fdifferential_privacy_laplace_01%2f&title=Differential%20Privacy%3a%20Primer%20%26%20Laplace%20Case%20Study"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Differential Privacy: Primer & Laplace Case Study on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fdlm.rocks%2fposts%2fdifferential_privacy_laplace_01%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Differential Privacy: Primer & Laplace Case Study on whatsapp" href="https://api.whatsapp.com/send?text=Differential%20Privacy%3a%20Primer%20%26%20Laplace%20Case%20Study%20-%20https%3a%2f%2fdlm.rocks%2fposts%2fdifferential_privacy_laplace_01%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Differential Privacy: Primer & Laplace Case Study on telegram" href="https://telegram.me/share/url?text=Differential%20Privacy%3a%20Primer%20%26%20Laplace%20Case%20Study&url=https%3a%2f%2fdlm.rocks%2fposts%2fdifferential_privacy_laplace_01%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>// Works by D.Moreno //</span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod rel="noopener noreferrer" target=_blank>Papermod</a> //
Hosted on
<a href=https://firebase.google.com/ rel="noopener noreferrer" target=_blank>Firebase</a> //</span></footer><a href=#top aria-label="Go to top" title="Scroll to top"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script>let menu=document.getElementById("menu");menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function s(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>for(var els=document.getElementsByClassName("highlight"),title,code,newNode,i=0;i<els.length;i++)title=els[i].title,code=els[i].firstElementChild.firstElementChild,title.length&&(newNode=document.createElement("div"),newNode.textContent=title,newNode.classList.add("highlight-title"),code.parentNode.insertBefore(newNode,code))</script></body></html>