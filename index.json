[{"content":"Why concerns about these new tools are echoes from previous generations and what they mean for the future of art and creativity.\n📌 TL;DR Art and creativity have historically benefited from advances in technology.\nAdvances in the tooling available for creative expression have invited a reevaluation of each medium\u0026rsquo;s place in the art world, and invite some reassessment of whether art made with the new tooling is ‘high art’.\nAI art will no more undermine the value of artists in our future than cameras have to paintings since their introduction in the 19th century.\nArt evolves in response to changing tooling 19th century impressionists were enabled by advances in portable painting tools (easels, premixed paints) Photography’s hyperrealism invited a shift towards abstraction in painting overtime Changes in art technology come with a democratization of… Art making: more people are able to make art as the tools become more accessible Advances like: Synthetic paints, premixed tube paints, cameras, digital creative studios / image editing apps, even mobile phone cameras - all helped get more people into art making. Ownership: manufacturable art and the proliferation of creative tooling all enabled by technology have gotten more people to ‘own’ art There is a history of debate around whether machines can be creative 1970's AARON was a robot that created drawings which were later painted over by the Cohen (artist, engineer) This consequential debate informs what we consider true AI, overtime this line has shifted in response to the evolving capabilities of current gen machine intelligence 📖 Book Recommendations Life3.0 by Max Tegmark for more on where we draw the line for human level AI, why it may have shifted over time, and how human level AI might develop in the future. The Age of AI by Eric Schmidt, Henry A. Kissinger, Daniel Huttenlocher for more on what it means to be human in a world shaped by AI. AI Wins Artfare, Community Pushes Back I feel like, right now, the art community is heading into an existential crisis if it’s not already. A big factor of that is … the disruptive technology of open AI. A lot of people are saying, ‘AI is never going to take over creative jobs, that’s never going to be something that artists and sculptors have to worry about.’ And here we are smack in the middle of it, dealing with it right now.\n— Jason Allen, Colorado State Art Contest Winner\nAllen\u0026#39;s First Place Entry in the Colorado State Art Contest This past September, Jason Allen entered a piece of digital art into the Colorado State Art Contest and won first place. Jason Allen revealed in a discord channel afterwards that he had used Midjourney, a text to image AI system, much to the dismay of others in the Midjourney discord community and many other folks online.\nThis sparked debate about whether Allen had cheated the contest by entering a work that wasn’t created by himself 1. This also opened up debates about whether AI generated works are art, and what place the tool has in the creative process moving forward. There have since been a number of communities that outright prohibit entries that were generated using AI and trending movements to promote banning AI art. Some in the art community have taken a more aggressive position, calling into question intriguing concerns around licensure of AI generated works on the basis that an AI creates based on information extracted from data in its training dataset. Of particular interest on the issues around licensing are prompts that solicit content from AI that is in the style of known artists or that mimic copyrighted material.\nNew Tech, Same Tensions Art and creativity have evolved alongside advances in technology. Some artistic movements were enabled by advances in the tooling and technology available to artists, and in other cases radical innovations in the tooling available to generate imagery have transformed prior mediums\u0026rsquo; place in the artistic world.\nAt each epoch of our creative tooling, there are concerns that the artistic vanguard are a threat to tradition and technique.\nArt continues to thrive, and some in these communities celebrate the release of new mediums as emerging methods for creative expression. As the accessibility of technology spreads through civilization we see that the new methods of creation carry a democratizing force: more people are able to participate in the art making and art ownership.\nTechnology and Art Advances in technology and movements in art are inextricably linked. As a result, the boundaries of what we consider creativity and art are also sensitive to the evolution of the technologies that enable their expression. Technological advances, from premixed paints to the proliferation of microprocessors, have democratized the creative process. Participants in traditional mediums and critics have resisted movements enabled by radical changes in artistic tooling and consequently invited a reevaluation of the line that separates creative expression from expressionless content.\nTube Paint Wielding Impressionists The romantic wave, which was ushered in with the 18th century literary movement, led to an era of imagery that focused on photorealistic imagery capturing a certain ideal, washed in tones that heightened their beauty and allure. Nonetheless, the images were praised for their minutiae, which highlighted those artists\u0026rsquo; technique and mastery of the depiction of our physical world, observed through our emotions. This movement was characterized by an imaginative finish to visual art that invited observers to connect deeply with the emotions captured in the imagery.\nIn the last quarter of the 19th century, artists in France popularized impressionist painting which favored the capture of raw, authentic imagery in lieu of fabricated scenes that had dominated the art world. Characteristic of this movement were the sketch-like, seemingly unfinished, paintings of unstaged scenes. Artists of this time explored painting ‘Plein Air’, opting to bring their creative process to the world around them as opposed to imagining contrived imagery from the comfort of their studios. The movement marked a shift from projecting idealism in imagery towards capturing the idiosyncratic happenings of real life: no longer were these artists comforted by controlled lighting, controlled setting, and controlled weather. By leaving these comforts, the Impressionist captured a particular moment faithfully albeit unrealistically.\nThe impressionist movement was enabled by technology.\nIn the mid-19th century commercial availability of portable art easels and an American oil painter John Goffe Rand’s invention: the portable syringe of premixed paint, enabled artists to bring their painting to the outdoor world 2. Artists began capturing minimalistic paintings of unstaged scenery, which also popularized landscape impressionist painting. Previously, outdoor painting was relegated to sketching while the true polish and expression of technique came in the studio. This was a necessity for any imagery that required a diverse palette because artists needed to meticulously mix dry pigment powders with organic oils to make the hues they sought to use in their works.\nThere was resistance to this change.\nCritics, along with the masters of Romantic idealism, dismissed early Impressionists and their works. The critique focused on the works appearing unfinished. The era’s namesake itself was an appropriation of Louis Leroy’s critique of a work by the now globally renowned Claude Monet: Impression, Sunrise 3.\nClaude Monet\u0026#39;s Impression, soleil levantPublished in Portfolio magazine, Art Foundation Press\nPhotography: An Art \u0026amp; Science Concurrent with the developing tensions between Impressionists and Romantics was the emergence and use of photography. This new medium emerged as a scientific tool where it was adopted in fields like botany and archaeology for its ability to faithfully capture the physical world in great detail 4.\nEarly photography used in physical sciencesPhotographs of British Algae: Cyanotype Impressions (1843)\nThis technology was adopted into broader use and it was received by an enthusiastic public that quickly took to the method of capturing subject matter with a level of accuracy that was otherwise unimaginable without years of formal training and practice. The early-adopting artists found the monochromatic medium a challenge: photographers grappled with obstacles like capturing texture and tone in grayscale. They also paved the way for the medium\u0026rsquo;s further adoption through experimentation and cataloging methods for capturing imagery and developing the photosensitive substrate. Pioneering photographers of these early days were squarely at the intersection of art and science.\nThere was resistance to this change.\nPhotography was a technological advance that fundamentally changed the availability and accessibility of tooling used to generate imagery. Previously, painters would be often tasked with capturing spaces, people, and time in imagery. A wealthy family wanting a family portrait contracted a skilled painter whose work would both capture and flatter their image. In these times such a portrait would become a mark of the family\u0026rsquo;s wealth, and be passed through generations as heirloom. In this context: photography replaced canvas with photoreactive metal sheets, the brush with a shutter, and the painter with a photographer.\nCritics of photography were plentiful. A photograph is unforgiving where the portrait painter would flatter their clients. Details painstakingly added to a landscape, demonstrating years of a technique’s mastery, were replicated with equivalent or higher detail by harnessing chemical properties and mechanical manipulation of light. The resistance came from: the traditional medium artists, art collectors, and art critics. For the artists the sentiment was that this new medium posed an existential threat to the craft. The critics gravitated towards photography’s process: the claim being that the medium lacked depth and spirit because the capture was mechanical and manufactured 5\nPhotography \u0026amp; Impressionism Intersect The collision of these mediums did not result in the slow fizzling-out of the old. Instead, the intersection of these technological and artistic advances invited a reimagination of each medium\u0026rsquo;s place in the world moving forward.\nThe introduction of a low barrier-to-use photorealistic medium resulted in a market flooded with precision manufactured imagery. It is argued that overtime this drove a trend towards abstraction in painting 6. This does not mean that photorealistic painting vanished; on the contrary, there continued to be a market for photorealism despite the introduction of a faster, cheaper, more accessible medium to capture reality.\nPhotography also evolved and its movements invited repeated debate as to its merit as an artform. It is generally accepted today that photography is an artform, though there are still some that argue about it. Movements in early photography included editing images into photo-collages that came in a spectrum from the pursuit for beauty all the way to absurdity:\nMixed Pickles, by Victoria Alexandrina Anderson-Pelham and Eva MacdonaldWestmorland Album, circa 1870\nBy Marie-Blanche-Hennelle FournierCollage of watercolor, ink, and albumen silver prints\nOthers took to editing the negatives of the images: selectively controlling exposure, layering negatives, and fine-tuning focus to create blurred prints. These effects turned photography into a craft, indeed a parallel to the craft of painting.\nThe Flatiron by Edward SteichenPrinted 1904, ref: Met Museum\nOvertime photography introduced a new medium for creatives to explore, and as more explored it the adoption lent itself to the merit of the medium as an art form.\nAI in Art There was a fear at the time of its introduction that photography might supplant painting as the technology improved 5. That may not have been realized, but the trajectory of painting as a medium was irrevocably impacted by the radical change in the tooling available to generate imagery that was brought on by the introduction of photography. Similar fears have arisen recently aimed at generative machine learning models: that take in text prompts and generate images.\nWhile this new application of AI has also changed the availability of image-creation tooling; the use of AI in art is not new.\nTooling available for photo and video editing has made use of machine learning models for much of their modern existence: single click background blurring, video editor object tracking, and redeye correction, these operations invoke image recognition software. More sophisticated editing software may propose likely edits intelligently based on the content of the image: edits including contrast, tone, image sharpening, filtering etc. The content recognition and proposed edits are the result of applied machine learning.\nTechnology in Art: A Democratizing Force Art has evolved in reaction to, or enabled by, technological changes in the tooling available to the artists. Furthermore, at each age in the tooling environment the new comes with a democratizing force that enables more people to participate in the art making and more people to participate in ownership of art.\nBefore the camera, only a select few would ever learn, or perhaps ever have the innate talent to, produce photorealistic painting. Photography made more accessible the tooling that could be used to produce such images. It also made having a family portrait no longer a mark of the elite class.\nThe invention of tube premixed paints launched a series of innovations: over time improved tube designs as well as the introduction of synthetic paints meant more variety, increased availability, and lower prices. Today, for roughly the price of a dinner out anyone can get started with an art easel, paint set, and some sketchpads.\nAI in art is also a democratizing force. It is now possible that anyone with at least internet access on a device with a screen is able to generate images in the style of any artist, of any era, of (almost) any subject matter. Just don’t ask it to draw hands 👌\nAI Rendering of Michelangelo-like PaintingStable Diffusion, prompt: \u0026lsquo;Painting on a cathedral roof in the style of Michelangelo\u0026rsquo;\nTechnology and Creativity The effect of technology on creativity is less clear. On one hand: we can say that new tooling has created new methods for creative expression. On the other: it seems that over time, and with more sophisticated technology, more of the creative process has been delegated to machines.\nFor instance, a craft perfected in dark rooms to edit negatives and create prints has been increasingly automated in digital creative studios. We’ve reached a point where our machine counter-parts will go as far as selecting the best of a series of burst photographs taken on your device, and then suggest the optimal edits for that image selected from the burst. All of this happens automatically, and the technology exists in your pocket. It would seem that just about the only thing we have not delegated to machines is deciding what to photograph.\nToday, the technology exists to generate photograph-like images of just about any scene, all from a text prompt. With generative ML we are left tussling with whether or not this AI is capable of being creative and what that means for the future of human artists. This encroaches on a philosophical debate rooted in the ambiguous definition of what is creativity, though exploring this debate will be left as an exercise for any motivated reader so inclined to dive in… I will; however, leave this out of the scope of this post.\nAn Early Creative Robot In the early 70’s a British born artist named Harold Cohen engineered a robotic system that generated imagery algorithmically. AARON, as the collection of algorithms and engineering projects came to be called, is ‘one of the longest running and continuously maintained AI systems in history’ 7.\nHarold Cohen \u0026amp; AARONHarold painting over a sketch produced by his creation AARON\nHarold applied color to the sketches the machine created. In doing so he created a man-machine artistic relationship. By the early 80’s the man-machine duo were making their mark in the art world by sharing their works in exhibitions and museums. Harold claimed that AARON was a system embedded with his own knowledge and that AARON was therefore not a creative machine but rather a tool available to Harold in his art making. Indeed, all but a few of the cocreated artworks were signed by the artist: Harold Cohen.\n\u0026ldquo;I think creativity is a relative term\u0026hellip; Clearly the machine is being creative, the program is being creative, to the degree that every time it does a drawing it does a drawing that nobody\u0026rsquo;s ever seen before including me. I don\u0026rsquo;t think it\u0026rsquo;s currently as creative as I am in writing the program, I think for a program to be fully creative, in a more complete sense creative, it has to be able to modify its own performance and that\u0026rsquo;s a very difficult problem\u0026rdquo;\n— Harold Cohen\nHarold Cohen, \u0026#39;Another Spring\u0026#39;Oil over pigment ink on canvas, triptych. 2011\nAs AARON and Cohen made their mark on the art world, their works invited great debate around the topic of whether AARON was / could be creative. As AARON’s feature set grew: first from sketching lines then to filling in its own works with color, the debate in favor of it being a creative machine gained some support.\nCan AI ever be Creative? A vision for artificial intelligence of yesteryear may have imagined a world with autonomous vehicles and HAL 9000 like sentient infrastructure. While today we would hardly consider a self-driving vehicle a human-equivalent artificial intelligence, despite its ability to solve for safe maneuvering in a risky real-time analog environment. Overtime we push our threshold for true artificial intelligence seemingly as soon as our systems have breached the previous definition, or maybe as soon as we become desensitized to the achievements of the current gen tech.\nThe boundary of what we consider true, human equivalent, AI has been periodically reevaluated.\nEither way: creativity, like human equivalent intelligence, seems to continually evade the reach of AIs. This could arguably be left to the fact that the criteria is somewhat nebulous, but it could also have some rooting in our collective resistance to attribute something we feel separates us from machines (and, perhaps, from other species) to objects of our own creation.\nAnd Will AI-Image Generation Always be Derivative? A common argument, in fact one that has raised interesting legal questions regarding licensure, is the question of whether image generating ML is derivative (and how derivative it is) in relation to the training data that was used to create the model.\nThese questions are intriguing, and beyond the scope of this post - though important.\nIf we would say that a system is derivative if its execution falls within the bounds of observed or encoded behavior, then it it important to acknowledge also that image generation may only be derivative for now.\nIn 1997, DeepBlue beat the world chess champion by executing expert chess strategies codified and run on purpose built and optimized architecture. We might say DeepBlue was derivative because it took human expert knowledge and applied it using overwhelming compute power.\nIn 2015, an artificial neural network developed by DeepMind Technologies called AlphaGo beat a world-class, professional Go player. This algorithm by the same metric was partially derivative: the algorithm was seeded with human Go games and human expert Go player strategies, and then used reinforcement learning to fine tune itself by playing additional games against itself.\nToday, the world’s best Go player is AlphaGo Zero. A neural network also developed by the same lab. The key difference here is that by the same metric this intelligence is not derivative. AlphaGo Zero was not seeded with human games and human strategies; instead it was trained entirely by playing games with itself. Like many first timers, it played Go terribly. In the first 3 days of its existence, AlphaGo Zero played almost 5 million games against itself. A few days later it achieved a proficiency that was comparable to global elite players 8.\nSystems like AlphaGo Zero require human researchers and engineers to define the rules and the boundaries of the sandbox, then the ML model is tasked to optimize for some objective function. Translating the creative process and purpose to codified rules and a sandbox may prove a challenge, but it is within the realm of possibility that an image generating AI may someday be non-derivative. If that day comes, the question of whether machines can be creative will come to term and our collective hand will be forced to make a decision.\nThe future of AI in Art \u0026amp; Creativity Art has evolved in reaction to shifts in the content creation tooling environment. This has necessarily invited a reevaluation of existing mediums’ strengths as the new tooling made cheaper, faster, more accurate, or some other dimension of content generation better than anyone could have previously imagined.\nMachine use has been shifted further left in the creative process during each advance. More of our creative processes are either enabled by or completely delegated to machines (e.g. picking the best picture from my burst, editing it, etc). Delegating more of the creative process to machines has allowed us to focus our attention on other, more intriguing matters (e.g. what is worthy of photographing).\nThere was a time when Socrates warned his pupils of the perils of writing; or rather, relying on the written word. In those times sharing knowledge was an oral exercise, and Socrates warned that relying on written text might make scholars’ minds lazy and forgetful. Of course this is draped in a rich irony: we only know of Socrates’s stance on written word through Plato’s writings that have survived many generations and translations. The availability of scribes and a consolidation of the Greek alphabet preserved Socrates’s thoughts at that time. Written word is to the academic tradition what these large-shifts in content creation tooling have been to art. The reliance on written word, and many other advances in technology, no more undermined scholarly pursuit than photography did to painting.\nIt’s important to frame our current concerns with ML generated art within the lens of our own history: the introduction of photography didn’t bring an end to painting, phonographs didn’t kill live performances, computers have become the best Go and Chess players on the planet yet these past-times are still enjoyed by many people globally.\nFinally, and maybe most importantly, each seismic shift in our creative tooling environment ushers in a greater accessibility to the creative process. As a result, more people are able to participate in the art making and art ownership.\nMattei, Shanti Escalante-De. “Ai-Generated Artwork Goes Viral after Winning Award at State Fair.” 1 Sept. 2022, https://www.artnews.com/art-news/news/colorado-state-fair-ai-generated-artwork-controversy-1234638022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n“En Plein Air - Modern Art Terms and Concepts.” The Art Story, https://www.theartstory.org/definition/en-plein-air.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSamu, Margaret. “Impressionism: Art and Modernity.” Metmuseum.org, Institute of Fine Arts, New York University, Oct. 2004, https://www.metmuseum.org/toah/hd/imml/hd_imml.htm.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLocke, Nancy. “How Photography Evolved from Science to Art.” The Conversation, 25 Dec. 2022, https://theconversation.com/how-photography-evolved-from-science-to-art-37146\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTeicher, Jordan G. “When Photography Wasn\u0026rsquo;t Art.” JSTOR Daily, 6 Feb. 2016, https://daily.jstor.org/when-photography-was-not-art.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSilva, Eva. “How Photography Pioneered a New Understanding of Art.” TheCollector, 24 June 2022, https://www.thecollector.com/how-photography-transformed-art.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGarcia, Chris. “Harold Cohen and Aaron-a 40-Year Collaboration.” Computer History Museum, 23 Aug. 2016, https://computerhistory.org/blog/harold-cohen-and-aaron-a-40-year-collaboration/.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKnapton, Sarah. “AlphaGo Zero: Google Deepmind Supercomputer Learns 3,000 Years of Human Knowledge in 40 Days.” The Telegraph, Telegraph Media Group, 18 Oct. 2017, https://www.telegraph.co.uk/science/2017/10/18/alphago-zero-google-deepmind-supercomputer-learns-3000-years/.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://dlm.rocks/posts/ai_art_wins_art_contest/","summary":"\u003cp\u003eWhy concerns about these new tools are echoes from previous generations and what they mean for the future of art and creativity.\u003c/p\u003e","title":"AI in Art \u0026 Creativity"},{"content":"This post serves as a primer to Differential Privacy: presenting an intuitive foundation for the definition, proceeding to some math, and finally presenting a case study to demonstrate some key concepts.\n📌 TL;DR Differential Privacy is a class of algorithms and a formal mathematical definition that, if met, provides an upper bound to the potential privacy loss for a query response (data release). The formulation stems from the intuition that we can reason about (and quantify) the privacy of some algorithm that shrouds in uncertainty whether or not a user\u0026rsquo;s data was in the dataset that informed the result. Differential Privacy is a definition There isn’t a single solution to meet the definition, instead there are multiple mechanisms that can be used Differentially Private algorithms allow us to quantify the privacy loss For any single query we are able to quantify the upper bound of the amount of secret information a response may reveal DP is composable, so for a series of queries we can quantify the total privacy loss / Proof / Epsilon is our measure of privacy loss For series of queries run over the same dataset- we can set a global epsilon value and budget across all the queries Lower epsilon = lower accuracy, higher noise, and greater privacy / Proof / and / Case Study/ The Theory: What is Differential Privacy The foundation of Differential Privacy (DP) is the notion that an individual’s privacy is protected in a dataset if the individual’s contribution is simply excluded from the set of records, assuming data is independent.\nConsider an algorithm that takes as input a database of user records, and outputs the average of all records. If this algorithm excluded from its calculation the contribution from a target user, or if the database did not include any contribution from that target user, then as long as records in the database were independent it follows that within the scope of what can be revealed by that algorithm the target’s data stays a secret. Specifically, DP concerns itself with shrouding in uncertainty a target’s participation in the dataset that produced the output that was released.\nDP algorithms are a class of algorithms that are derived from this intuition, and that carry a mathematical guarantee for the upper-bound privacy loss of each query or data release. This mathematical formulation was the culmination of decades of privacy research, which demonstrated that prior privacy preserving standards were leaky: with enough time and repeated releases it is infeasible to make any lasting guarantee about privacy. 1 This insight motivated the development of a new standard which stemmed from the missing member in database intuition and was presented in the seminal work by C. Dwork et al. “Calibrating Noise to Sensitivity in Private Data Analysis”. 2 In their work they introduced the mathematics to describe a DP algorithm, as well as a generalized approach to implementation which, in its original or derivative forms, has since become recognized as the gold standard for privacy preserving data transformations.\nFrom an adversarial point of view, DP roughly means that for a given query output we would be uncertain (quantifiably so) that the output was derived from a database that did or did not contain a target user’s data. Paying homage to the cryptographic roots of privacy research and differential privacy, this is analogous to semantic security in a cryptosystem.\nWhy was DP groundbreaking? DP departed from previous attempts at generalized privacy preserving algorithms and standards notably with two characteristics:\nA quantification of the upper-bound privacy loss that might result for a data release Formalization of the privacy loss for repeated data releases from the same source data The second of these observations is a result of DP being composable, this property allows data curators to set a global maximum privacy budget which is divided among the applications of the confidential data - the proof and practical interpretation on this later in this post.\nTogether, these new characteristics of the data processing mean that a system that adheres to the strict mathematical definition that is DP allows us to reason about and quantify the privacy risk against privacy attacks today as well as into the future. 3\nThe Math: Derivation of Differential Privacy An algorithm \\(\\mathbb{A}\\) is said to satisfy epsilon Differential Privacy (εDP) 2 if for all subsets \\(S\\) in the range \\(\\mathbb{A}\\) , for the databases \\( D_1 \\) and \\( D_2 \\) where [ \\( D_1, D_2\\) ] differ in at most the contribution of a single member (alternatively, differ in at most an arbitrary perturbation of a single record): $$ \\tag{1} Pr \\Big(\\mathbb{A}(D_1) \\in S \\Big) \\leq\t\\ e ^ \\epsilon \\cdotp\tPr \\Big(\\mathbb{A}(D_2) \\in S \\Big) $$\nWhere (\\(\\epsilon\\) \u003e 0). Epsilon (\u0026epsilon;) is called the privacy loss or privacy budget. Rearranging this gives some insight to what epsilon means: $$ \\tag{2} \\frac {Pr\\Big(\\mathbb{A}(D_1) \\in S \\Big)} {Pr \\Big(\\mathbb{A}(D_2) \\in S \\Big)}\\ \\leq e ^ \\epsilon $$\nThis rearranged form reveals that (\\(\\epsilon\\)) is at most a function of the ratio of probabilities that an observation resulted from either \\( D_1 \\) or \\( D_2 \\). The expression in Equation 01 is absolute for the entire space of the algorithm’s output, and it is this strict definition that affords the guarantee that an adversary is limited to the upper bound over the whole range of \\(\\mathbb{A}\\). In practice, this strict definition may be relaxed with the addition of a privacy error term δ 4 :\n$$ \\tag{3} Pr \\Big(\\mathbb{A}(D_1) \\in S \\Big) \\leq\t\\ e ^ \\epsilon \\cdotp\tPr \\Big(\\mathbb{A}(D_2) \\in S \\Big) + \\delta $$\nThis error term can be roughly interpreted as the likelihood that the strict definition doesn’t hold. For brevity’s sake, I will introduce this here and leave as a teaser the fact that the error term can exist and that it exists to balance engineering practicality with the otherwise strict DP definition (besides… I need to save some content for future idea sharing posts! 😉)\nA quick proof… Quantifying Privacy Loss for Repeated Queries In order to reason about the upper bound for repeated queries we need to prove that εDP is sequentially composable:\nLet \\(\\mathbb{A}_1\\) and \\(\\mathbb{A}_2\\) be algorithms that satisfy Equation 01 and are thus \u0026epsilon;DP with \\(\\epsilon_1\\) and \\(\\epsilon_2\\) privacy loss parameters respectively. Let databases \\( D_1 \\) or \\( D_2 \\) differ by at most 1 subject's contribution, and whose output are spaces \\(\\mathcal{R_1} \\) and \\(\\mathcal{R_2}\\). So far we have: $$ \\tag{4.1} Pr \\Big(\\mathbb{A_1}(D_1) \\in \\mathcal{R_1} \\Big) \\leq\t\\ e ^ {\\epsilon_1} \\cdotp\tPr \\Big(\\mathbb{A_1}(D_2) \\in \\mathcal{R_1} \\Big) $$ $$ \\tag{4.2} Pr \\Big(\\mathbb{A_2}(D_1) \\in \\mathcal{R_2} \\Big) \\leq\t\\ e ^ {\\epsilon_2} \\cdotp\tPr \\Big(\\mathbb{A_2}(D_2) \\in \\mathcal{R_2} \\Big) $$\nIf a third algorithm, \\(\\mathbb{C}\\), is a combination of both ( \\(\\mathbb{A}_1, \\mathbb{A}_2\\) ) such that \\(\\mathbb{C} \\to \\mathcal{R_1} × \\mathcal{R_2} \\). Then, for some algorithm output ( \\(\\mathbb{A}_1 \\to r_1\\) ) and (\\(\\mathbb{A}_2 \\to r_2\\) ) where ( \\(r_1, r_2 ) \\in \\mathcal{R_1} × \\mathcal{R_2} \\) then we can state for \\(\\mathbb{C}\\): $$ \\tag{4.3} \\frac {Pr \\Big(\\mathbb{C}(D_1) = (r_1, r_2 ) \\Big)} {Pr \\Big(\\mathbb{C}(D_2) = (r_1, r_2 ) \\Big)} = \\frac {Pr \\Big(\\mathbb{A_1}(D_1) = r_1 \\Big)} {Pr \\Big(\\mathbb{A_1}(D_2) = r_1 \\Big)} \\frac {Pr \\Big(\\mathbb{A_2}(D_1) = r_2 \\Big)} {Pr \\Big(\\mathbb{A_2}(D_2) = r_2 \\Big)} $$\nBy a matter of substitution using Equation 2, we have:\n$$ \\tag{4.4} \\frac {Pr \\Big(\\mathbb{C}(D_1) = (r_1, r_2 ) \\Big)} {Pr \\Big(\\mathbb{C}(D_2) = (r_1, r_2 ) \\Big)} \\leq e ^ {\\epsilon_1} e ^ {\\epsilon_2} $$\n$$ \\tag{4.4} \\frac {Pr \\Big(\\mathbb{C}(D_1) = (r_1, r_2 ) \\Big)} {Pr \\Big(\\mathbb{C}(D_2) = (r_1, r_2 ) \\Big)} \\leq e ^ {\\epsilon_1 + \\epsilon_2} $$\nThis can be generalized for any algorithm that is the composite of N independent differentially private algorithms:\n$$ \\tag{4.5} \\frac {Pr \\Big(\\mathbb{C}(D_1) = (r_1, r_2, \u0026hellip; r_n) ) \\Big)} {Pr \\Big(\\mathbb{C}(D_2) = (r_1, r_2, \u0026hellip; r_n) \\Big)} \\leq e ^ {\\epsilon_1 + \\epsilon_2 + \u0026hellip; + \\epsilon_n} $$\nThe sum of all privacy losses is the global privacy loss, which can be divided between the N algorithms as needed. The composability of DP tells us that theoretically we can assert that at most privacy erodes linearly with respect to n-epsilons, though in practice it is sublinear.\nAnd a bit more math… Poking at ε Some reasoning about how to set ( \\(\\epsilon \\) ), and what it means for the analysis is readily developed by poking at Equation 02 with limits: Let \\(\\epsilon \\rightarrow 0\\) $$ \\tag{2} \\Bigg( \\frac {Pr\\Big(\\mathbb{A}(D_1) \\in S \\Big)} {Pr \\Big(\\mathbb{A}(D_2) \\in S \\Big)} \\leq { e ^ \\epsilon } \\Bigg) {\\Bigg\\vert _{\\epsilon \\rightarrow 0}} $$\n$$ \\tag{2.1} \\frac {Pr\\Big(\\mathbb{A}(D_1) \\in S \\Big)} {Pr \\Big(\\mathbb{A}(D_2) \\in S \\Big)} \\leq 1 $$\nFor the case where \\(\\epsilon \\rightarrow 0\\), for some given output in range \\(S\\) we are at most equally likely to deduce that the output has come from \\(D_1\\) or \\(D_2\\). This means that we have maximized the uncertainty as to which dataset informed the output. Within the context of the opening intuition: maximum uncertainty is maximum privacy. In Practice DP’s definition is really a conceptual property that describes a data transformation. There are a few potential transformations that satisfy this property, some popular mechanisms are additive noise mechanisms which sample noise from a distribution and inject that noise into the query response. Some popular additive noise mechanisms are:\nLaplace Mechanism This mechanism is the canonical DP mechanism that satisfies ε-DP (Equation 01), and is otherwise known as ‘pure differential privacy’. This mechanism samples noise from a zero-centered Laplace distribution and injects this noise into the data release. 2\nGaussian Mechanism The Gaussian mechanism is very similar to the Laplace mechanism, except it uses a zero-centered, variance bound Guassian distribution and does not satisfy ε-DP, instead it satisfies (ε, δ) Differential Privacy (Equation 03). In addition to restrictions on the Noise-Distribution’s variance, this additive noise mechanism only holds true to (ε, δ) DP when ϵ \u0026lt; 1.\n🔨 Implementation tips Differential Privacy is a powerful tool, while wielding it watch for:\nDimensions in the source data that are correlated: this leads to leaky privacy guarantees Extreme outliers in the data: users in the long tails of data are more identifiable, require more noise to mask. Consider binning data into ranges to address this. This is also the subject of the case study DP with Privacy Budget Accounting Composition has been demonstrated for εDP algorithms, which allows us to define a global limit on the maximum privacy loss that we will allow for a given dataset. When this global privacy loss is set, multiple queries are each assigned some portion of the whole budget. In other words, repeated queries deplete the global privacy budget.\nReferring back to Equation 02 - note that for lower ε values we expect more uncertainty in which of the datasets the algorithm’s output came from D1 or D2. As a generalization we have:\nLower ε value:\nHigher the noise we expect in the query response Lower accuracy in the query response as a result of the noise Better privacy story, but at the expense of the data utility 🔧 ε Setting Tips Fixing the global privacy budget means that care is needed when accounting for where to allocate higher accuracy queries, or where a sacrifice on accuracy may be made to stay within the budget.\nIn addition to careful accounting, cached answers can extend the budget. During the fulfillment of the query-response protocol: cache answers to queries, and retrieve cached answers when the same query is issued multiple times to the database. This type of response-replay won’t deplete the privacy budget, and ensures no additional information is learned about the data’s subjects for repeated queries.\nCase Study The following case study is hosted on GitHub, which includes a Creative Commons licensed dataset (thanks Kaggle!). In this case study we will be exploring the implementation of OpenMined PyDP which is a python wrapper that makes use of Google’s open source C++ Differential Privacy library. This Python library uses the Laplace mechanism of additive noise to satisfy εDP.\nObjective: In this case study we will see the effect that:\nDifferent ε values have on the differential privacy noise Long tail skew in the source data have on differential privacy noise The Data For the fulfillment of this case study the data will first need to be:\nSanitized (e.g. correct string entries in numeric columns, etc) Columns dropped (the data cube diced) In addition to this clean up, we can drop all but the last month’s data for simplicity\u0026rsquo;s sake (data cube sliced). For the remainder of the case study we will be using the ‘Annual_Income‘ dimension only.\nAnnual Income plotted in linear and log scale This is a terrific dimension to explore: it exhibits a rich (pun intended 💰), long tailed right skew. Note that the tail is so long that it is impractical to plot annual income in linear scale (left), in log-scale we get a greater sense of where most users are and where only a few, very wealthy users are.\nThe Private Queries The Git Python project includes methods for private queries:\nRepeated_average Repeated_sum Repated_max Which take as input:\niterations (integer) number of times the private query will be repeated privacy_budget (float) epsilon value, does not change between iterations list (array) dataframe dimension to list, that is used as source data for the private query These methods do not use any privacy loss budget accounting, as a result over N-Iterations we expect a nice Laplace distribution in the observed query responses that should center on the true value.\nNoise \u0026amp; ε Running the repeated average method over 100 iterations, with ε = [1,100] returns the following histograms of observed query respones:\nRepeated Private Average Queries, 100 iterations with Epsilon = 100 Repeated Private Average Queries, 100 iterations with Epsilon = 1 As expected, a higher ε value yields query responses with lower noise injected - demonstrated by a distribution with a tighter spread, both distributions are centered on the true value which is represented by a vertical line.\nNoise \u0026amp; Long-tailed data We can imagine that blending in at a crowded event is much easier than trying to blend in in an otherwise empty room; so too there is a certain degree of privacy protection among a concentrated band of data rather than in a long tail. In other words, being a member of the 107 annual income club makes it difficult to apply enough noise to mask your contribution in the annual income dataset when running a private average query.\nRunning repeated private queries with iterations = 1000 and ε = 0.85 for the raw dataset (right skew, without any skew correction) and again for a skew-corrected dataset (still right skew, but cutting off anyone that isn’t below the 95% percentile).\nFor skew correction, we want to keep the count of unique users (in the dataset, count of rows) the same between datasets to make an apple to apple comparison. To address this, the skew-correction method takes any entry above 95 percentile, and replaces their contribution to annual income to be something within the sub-95 percentile band (using a random number generator within a bound).\nNormalized noise observations for repeated private queries, 1000 iterations, epsilon = 0.85 These histograms are normalized to their respective distributions\u0026rsquo; average salary. Eliminating the long tail results in over a degree of magnitude change for the normalized noise distributions. 👀\nIrit Dinur and Kobbi Nissim, Revealing information while preserving privacy\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith, Calibrating noise to sensitivity in private data analysis\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKobbi Nissim , Thomas Steinke , Alexandra Wood, Micah Altman, Aaron Bembenek, Mark Bun, Marco Gaboardi, David R. O’Brien, and Salil Vadhan, Differential Privacy: A Primer for a Non-technical Audience\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor, Our data, Ourselves: Privacy via Distributions Noise Generation\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://dlm.rocks/posts/differential_privacy_laplace_01/","summary":"\u003cp\u003eThis post serves as a primer to Differential Privacy: presenting an intuitive foundation for the definition, proceeding to some math, and finally presenting a case study to demonstrate some key concepts.\u003c/p\u003e","title":"Differential Privacy: Primer \u0026 Laplace Case Study"},{"content":"In this inaugural post I walk through the decision making that informed the design of this site (which tools and technologies I used), and the steps to set it all up.\n📌 TL;DR This site is\nhosted on Firebase created using Hugo has a custom domain purchased on Google Domains Built following these steps Lessons / Key takeaways Start projects with a goal, list of requirements and constraints. Then consider the options before starting on any work. I knew I wanted to set up my own site, because I’ve been wanting to learn the basics of front end dev and have a space to play around in At first, I started using Octopress for content generation then double backed to research alternatives before landing on Hugo. If I had researched first, I would have saved some time. This site is hosted on Firebase It has a free tier with plenty of headroom before I hit any price point Offers a nice runway to grow the site beyond static content, with serverless backends and plenty of plugins to try in the future For custom domains purchased on Google Domains, the ‘ownership check’ is skipped which saves time and implementation complexity The content is generated using Hugo Hugo has an active community (reddit, discord, and thorough documentation) which is reassuring for a FE beginner It’s written in Go, which is also something on my #ToLearn list Planning the build I followed these steps to get this site up:\nStart with a goal, and list of requirements and constraints Generate the site content Find a host and custom domain First: Goals, requirements, and constraints TL;DR If you want to focus on writing, there is no need to set up your own site. Go with Medium, which is the best free-to-post site IMO.\nI had intended to learn some FE basics + start writing this year. Setting up this blog allowed me to peel two potatoes at the same time (cruelty free figures of speech only on this site 🐦 🐦)\nGoals Find a place to host my writing Learn HTML, Javascript, and CSS basics Host in a place that allows me to link to github projects Stretch: Run code from the site itself Non-goals Build a dynamic site (\u0026hellip;yet) / build anything that would necessitate a database Requirements I’m new to front end development, so I need something that comes with a framework I can tinker with as opposed to building something from scratch The framework should come with an active community so that I can find support as needed I am not interested in opening my home network to the internet, so I’d like to host on a third party platform Preferably a free-to-host platform Constraints Time: My primary goal is to have a place to write, but my secondary goal is having a sandbox to explore and configure a site. A tension arises here as the tinkering pulls time out of writing, and vice versa. Second, generate the content TL;DR I chose Hugo because it had the most active community for support and it’s written in Go which is a language I wanted to pick up anyway.\nI went on the hunt for static content generators, which are tools to create the html, css, and javascript that would become my site. In doing so, I found a new requirement that the site be tolerable aesthetically so a generator with an active community and support for themes was added to my list from the requirements step.\nI found this great list of site generators, from there I narrowed down to a few options based on the technologies used as well as the communities supporting the tooling:\nSite Content Options Description Comments Eleventy Open sourced Javascript static site generator Handles incremental builds Has an active Git, good documentation, and active Discord Many actively maintained plugins Requires a bit more setup for hosting on GitHub Pages \u0026 Firebase (Anecdotally) Next.js Minimalist Jamstack framework using Javascript, typescript, and rust. Objective is to build scalable React web apps (static, dynamic) Good community, popular modern framework Lack of native themes Lacks plugins (e.g. 'read time' was a feature I was after on my site) Steep learning curve compared to plug and play generators Hugo Written in Go General purpose static site framework Handles incremental builds Fastest builder (thanks Go!) Limited plugins Many community supported themes Steep learning curve to create new themes (thanks Go :frown:) but many themes already to choose from. I went with Hugo, because it offered a quick setup with plenty of room for tinkering after the initialization. It’s also written in Go which makes site building fast for quick prototyping down the road.\nThird, finding a host TL;DR I went with Firebase which offers trivial custom domain setup along with a free tier that offers plenty of headroom for the traffic and site content I plan to create.\nI considered the following for hosting - I initially published to GitHub Pages; which seemingly offered the fastest time-to-deployment since my source was in a public repo anyway, but switched to Firebase after configuring my custom domain. Firebase, I found, offered the most runway for continued development beyond static content only (no concrete plans here, but the optionality is valuable).\nHost Options Description Comments Netlify Serverless Jamstack platform. Free tier up to 300 build minutes per month. Support for web dev CI/CD Many 3P service APIs 1-click roll backs Github Pages Git repo hosting service that may run the files through a jekyll build process. Soft build limit (10/hour), and site size (1gb served, 100gb bandwidth) Git provided cert 🔒 Zero-config build Firebase App and rich site development platform, offers automated backend services and a suite of features for release and app measurement. Has a free tier also Zero-config SSL certs 🔒 Trivial setup for real-time backend 1-click roll backs GCP backed for serving at scale (GCP comes with $300 new acct credits) Impressive open-sourced bundled features NOTE: Not an exhaustive list, see more complete list here.\nFinally, getting a neat custom domain Pricing was a wash and there is no differentiator in the product offering itself (no matter the seller, you’re getting the same domain!) I went with Google Domains because when adding the custom domain in Firebase, if purchased from Google Domains then the ownership proof is done automatically which will save some time and implementation compleixty.\nNOTE: GitHub Pages threw an error when I tried to reconfigure the build to my custom domain, so I chose to relocate my site to Firebase at this point.\nBuilding it I will skip steps for hosting on GitHub Pages, which can otherwise be found here, and provide the steps I followed to host on Firebase (including setting up my custom domain name on Google Domains)\nPrerequisites: Git, Hugo, and Node.js\nbrew install git brew install hugo brew install node Setup a Github repo for the site\nCreate a repo with the naming convention [username].github.io\nI left “main” as the source for Hugo build and created a separate branch for the site called gh-pages NOTE: this was initially for the github pages deployment, but I kept the naming convention.\nClone the github repo locally.\nyou’ll want to be working locally from the main branch Initialize Hugo in your cloned repo.\nhugo new site [username].github.io/ cd [username].github.io/ Picking a theme requires downloading \u0026amp; updating the config file. I am using PaperMod\nNOTE: PaperMod instructions use a YAML based Hugo build but I kept TOML. You can find YAML → TOML translators online. A sample YAML config for PaperMod is here\ngit clone https://github.com/adityatelange/hugo-PaperMod themes/PaperMod --depth=1 configure only the contents of ./public/* (the built site) to push to the gh-pages branch mkdir .github/ .github/workflows touch .github/workflows/ Add the following content to that file:\nname: github pages on: push: branches: - main # Set a branch to deploy pull_request: jobs: deploy: runs-on: ubuntu-22.04 steps: - uses: actions/checkout@v3 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#39;latest\u0026#39; # extended: true - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 if: github.ref == \u0026#39;refs/heads/main\u0026#39; with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_dir: ./public Now, the local working copy is ready for prototyping!\nCreate a new post and view it locally to test Hugo and your new theme.\nhugo new posts/HelloWorld.md echo \u0026#34;Hello World!\u0026#34; \u0026gt;\u0026gt; posts/HelloWorld.md hugo server -D Pro-Tip: you can draft content in Google Docs, then use a Markdown converter extension called “Docs to Markdown”. Otherwise working from and IDE does the trick also.\nSetting up a custom domain for Firebase project Create a Firebase project for the site Purchase an available domain, let’s say somenamehere.com/ Add custom domain to Firebase hosting In Firebase project console, select Hosting from left nav menu Under the project domains, select “Add custom domains” Add an entry for www.somenamehere.com Add an entry for somenamehere.com, select “redirect to an existing website” and redirect to www.somenamehere.com Note the IP addresses assigned at these steps, as well as the record types (e.g. “A”) Firebase HostingAdding custom domain to Firebase hosting\nRegister DNS, return to Google Domains From Google Domains console, select DNS from left nav menu Under “Custom Records” select “Manage custom records” Enter in the custom domain somenamehere.com and www.somenamehere.com here along with the noted IP addresses and the record types. Google DomainsUpdating DNS Register\nInitializing Firebase and deploying the site Initialize Firebase at the root blog directory, in this example it’s been [username].github.io npm install -g firebase-tools firebase login # will be prompted for firebase login in browser firebase init In addition to the selections noted here,\nselect gh-pages as the source for the site during setup Now, the site can be deployed 🚀\nhugo \u0026amp;\u0026amp; firebase deploy Don\u0026rsquo;t forget to push your built site to Git 💾\n","permalink":"https://dlm.rocks/posts/how_this_all_started/","summary":"\u003cp\u003eIn this inaugural post I walk through the decision making that informed the design of this site (which tools and technologies I used), and the steps to set it all up.\u003c/p\u003e","title":"How this all started..."},{"content":"Welcome! 👋 Hi, you\u0026rsquo;ve wandered to the about page.\nAbout the Site Welcome to David\u0026rsquo;s idea sharing site. Here you will find ramblings about topics like:\nprivacy \u0026amp; cybersecurity ML \u0026amp; data science Neat books Good podcast recommendations Maybe some artwork too *Note: topics list is growing and subject to change\nRecently, infrared radiation was recorded from the deepest pocket of space we\u0026rsquo;ve seen (albeit for a small slice) of our known Universe. It took in the order of 1010 years for this information to reach your computer screen, I commit to posting at least more often than it has taken for this deep space to reach you 👍\nDeep UniversePhoto by NASA’s James Webb Telescope\nA note on privacy \u0026hellip; just when you thought you\u0026rsquo;d get away without some privacy talk \u0026hellip;\nThis site uses Google Analytics, I use this data to understand:\nwhat content my audience is engaging with (e.g. which pages are visited) who my audience is, in coarse terms (e.g. coarse geotag) These data are used to measure content engagement.\nThis site does not\nhave a subscribe feature comments or other content submission forms include referrer in share links This site is also open source, so you can independently verify this 🙌\nAbout the Author Greetings.\nI am David, born and raised in the Minneapolis area, and currently living in California.\nI enjoy spending my time learning new things, spending time outdoors, and spending time with my family. Discovering activities that allow for all three at the same time is the sweet spot, if you got leads on this please let me know. Here is a fun pseudonymized pictured of myself next to an Alpaca.\nPixelated engineer \u0026amp; an alpaca, Circa 2019Engineer makes poor attempt at privacy joke in \u0026lsquo;about me\u0026rsquo; section\nIn my free time I\u0026rsquo;m excited to write about stuff I find interesting, \u0026amp; I hope others find neat too.\nWant more about me? I\u0026rsquo;ll keep it short:\nLikes Traveling and trying new foods Meeting people\u0026rsquo;s pets 🐶 🐱\u0026hellip; esp. those with human names Music, all kinds Dislikes lists without at least one item on them Thanks for checking out my blog / site. I hope you find some of this material educational, or at least entertaining.\nEnjoy your stay.\n- D\n","permalink":"https://dlm.rocks/about/","summary":"Welcome! 👋 Hi, you\u0026rsquo;ve wandered to the about page.\nAbout the Site Welcome to David\u0026rsquo;s idea sharing site. Here you will find ramblings about topics like:\nprivacy \u0026amp; cybersecurity ML \u0026amp; data science Neat books Good podcast recommendations Maybe some artwork too *Note: topics list is growing and subject to change\nRecently, infrared radiation was recorded from the deepest pocket of space we\u0026rsquo;ve seen (albeit for a small slice) of our known Universe.","title":"About"}]