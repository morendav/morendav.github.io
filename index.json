[{"content":"This post serves as a primer to Differential Privacy: presenting an intuitive foundation for the definition, proceeding to some math, and finally presenting a case study to demonstrate some key concepts.\nüìå TL;DR Differential Privacy is a class of algorithms and a formal mathematical definition that, if met, provides an upper bound to the potential privacy loss for a query response (data release). The formulation stems from the intuition that we can reason about (and quantify) the privacy of some algorithm that shrouds in uncertainty whether or not a user\u0026rsquo;s data was in the dataset that informed the result. Differential Privacy is a definition There isn‚Äôt a single solution to meet the definition, instead there are multiple mechanisms that can be used Differentially Private algorithms allow us to quantify the privacy loss For any single query we are able to quantify the upper bound of the amount of secret information a response may reveal DP is composable, so for a series of queries we can quantify the total privacy loss / Proof / Epsilon is our measure of privacy loss For series of queries run over the same dataset- we can set a global epsilon value and budget across all the queries Lower epsilon = lower accuracy, higher noise, and greater privacy / Proof / and / Case Study/ The Theory: What is Differential Privacy The foundation of Differential Privacy (DP) is the notion that an individual‚Äôs privacy is protected in a dataset if the individual‚Äôs contribution is simply excluded from the set of records, assuming data is independent.\nConsider an algorithm that takes as input a database of user records, and outputs the average of all records. If this algorithm excluded from its calculation the contribution from a target user, or if the database did not include any contribution from that target user, then as long as records in the database were independent it follows that within the scope of what can be revealed by that algorithm the target‚Äôs data stays a secret. Specifically, DP concerns itself with shrouding in uncertainty a target‚Äôs participation in the dataset that produced the output that was released.\nDP algorithms are a class of algorithms that are derived from this intuition, and that carry a mathematical guarantee for the upper-bound privacy loss of each query or data release. This mathematical formulation was the culmination of decades of privacy research, which demonstrated that prior privacy preserving standards were leaky: with enough time and repeated releases it is infeasible to make any lasting guarantee about privacy. 1 This insight motivated the development of a new standard which stemmed from the missing member in database intuition and was presented in the seminal work by C. Dwork et al. ‚ÄúCalibrating Noise to Sensitivity in Private Data Analysis‚Äù. 2 In their work they introduced the mathematics to describe a DP algorithm, as well as a generalized approach to implementation which, in its original or derivative forms, has since become recognized as the gold standard for privacy preserving data transformations.\nFrom an adversarial point of view, DP roughly means that for a given query output we would be uncertain (quantifiably so) that the output was derived from a database that did or did not contain a target user‚Äôs data. Paying homage to the cryptographic roots of privacy research and differential privacy, this is analogous to semantic security in a cryptosystem.\nWhy was DP groundbreaking? DP departed from previous attempts at generalized privacy preserving algorithms and standards notably with two characteristics:\nA quantification of the upper-bound privacy loss that might result for a data release Formalization of the privacy loss for repeated data releases from the same source data The second of these observations is a result of DP being composable, this property allows data curators to set a global maximum privacy budget which is divided among the applications of the confidential data - the proof and practical interpretation on this later in this post.\nTogether, these new characteristics of the data processing mean that a system that adheres to the strict mathematical definition that is DP allows us to reason about and quantify the privacy risk against privacy attacks today as well as into the future. 3\nThe Math: Derivation of Differential Privacy An algorithm \\(\\mathbb{A}\\) is said to satisfy epsilon Differential Privacy (ŒµDP) 2 if for all subsets \\(S\\) in the range \\(\\mathbb{A}\\) , for the databases \\( D_1 \\) and \\( D_2 \\) where [ \\( D_1, D_2\\) ] differ in at most the contribution of a single member (alternatively, differ in at most an arbitrary perturbation of a single record): $$ \\tag{1} Pr \\Big(\\mathbb{A}(D_1) \\in S \\Big) \\leq\t\\ e ^ \\epsilon \\cdotp\tPr \\Big(\\mathbb{A}(D_2) \\in S \\Big) $$\nWhere (\\(\\epsilon\\) \u003e 0). Epsilon (\u0026epsilon;) is called the privacy loss or privacy budget. Rearranging this gives some insight to what epsilon means: $$ \\tag{2} \\frac {Pr\\Big(\\mathbb{A}(D_1) \\in S \\Big)} {Pr \\Big(\\mathbb{A}(D_2) \\in S \\Big)}\\ \\leq e ^ \\epsilon $$\nThis rearranged form reveals that (\\(\\epsilon\\)) is at most a function of the ratio of probabilities that an observation resulted from either \\( D_1 \\) or \\( D_2 \\). The expression in Equation 01 is absolute for the entire space of the algorithm‚Äôs output, and it is this strict definition that affords the guarantee that an adversary is limited to the upper bound over the whole range of \\(\\mathbb{A}\\). In practice, this strict definition may be relaxed with the addition of a privacy error term Œ¥ 4 :\n$$ \\tag{3} Pr \\Big(\\mathbb{A}(D_1) \\in S \\Big) \\leq\t\\ e ^ \\epsilon \\cdotp\tPr \\Big(\\mathbb{A}(D_2) \\in S \\Big) + \\delta $$\nThis error term can be roughly interpreted as the likelihood that the strict definition doesn‚Äôt hold. For brevity‚Äôs sake, I will introduce this here and leave as a teaser the fact that the error term can exist and that it exists to balance engineering practicality with the otherwise strict DP definition (besides‚Ä¶ I need to save some content for future idea sharing posts! üòâ)\nA quick proof‚Ä¶ Quantifying Privacy Loss for Repeated Queries In order to reason about the upper bound for repeated queries we need to prove that ŒµDP is sequentially composable:\nLet \\(\\mathbb{A}_1\\) and \\(\\mathbb{A}_2\\) be algorithms that satisfy Equation 01 and are thus \u0026epsilon;DP with \\(\\epsilon_1\\) and \\(\\epsilon_2\\) privacy loss parameters respectively. Let databases \\( D_1 \\) or \\( D_2 \\) differ by at most 1 subject's contribution, and whose output are spaces \\(\\mathcal{R_1} \\) and \\(\\mathcal{R_2}\\). So far we have: $$ \\tag{4.1} Pr \\Big(\\mathbb{A_1}(D_1) \\in \\mathcal{R_1} \\Big) \\leq\t\\ e ^ {\\epsilon_1} \\cdotp\tPr \\Big(\\mathbb{A_1}(D_2) \\in \\mathcal{R_1} \\Big) $$ $$ \\tag{4.2} Pr \\Big(\\mathbb{A_2}(D_1) \\in \\mathcal{R_2} \\Big) \\leq\t\\ e ^ {\\epsilon_2} \\cdotp\tPr \\Big(\\mathbb{A_2}(D_2) \\in \\mathcal{R_2} \\Big) $$\nIf a third algorithm, \\(\\mathbb{C}\\), is a combination of both ( \\(\\mathbb{A}_1, \\mathbb{A}_2\\) ) such that \\(\\mathbb{C} \\to \\mathcal{R_1} √ó \\mathcal{R_2} \\). Then, for some algorithm output ( \\(\\mathbb{A}_1 \\to r_1\\) ) and (\\(\\mathbb{A}_2 \\to r_2\\) ) where ( \\(r_1, r_2 ) \\in \\mathcal{R_1} √ó \\mathcal{R_2} \\) then we can state for \\(\\mathbb{C}\\): $$ \\tag{4.3} \\frac {Pr \\Big(\\mathbb{C}(D_1) = (r_1, r_2 ) \\Big)} {Pr \\Big(\\mathbb{C}(D_2) = (r_1, r_2 ) \\Big)} = \\frac {Pr \\Big(\\mathbb{A_1}(D_1) = r_1 \\Big)} {Pr \\Big(\\mathbb{A_1}(D_2) = r_1 \\Big)} \\frac {Pr \\Big(\\mathbb{A_2}(D_1) = r_2 \\Big)} {Pr \\Big(\\mathbb{A_2}(D_2) = r_2 \\Big)} $$\nBy a matter of substitution using Equation 2, we have:\n$$ \\tag{4.4} \\frac {Pr \\Big(\\mathbb{C}(D_1) = (r_1, r_2 ) \\Big)} {Pr \\Big(\\mathbb{C}(D_2) = (r_1, r_2 ) \\Big)} \\leq e ^ {\\epsilon_1} e ^ {\\epsilon_2} $$\n$$ \\tag{4.4} \\frac {Pr \\Big(\\mathbb{C}(D_1) = (r_1, r_2 ) \\Big)} {Pr \\Big(\\mathbb{C}(D_2) = (r_1, r_2 ) \\Big)} \\leq e ^ {\\epsilon_1 + \\epsilon_2} $$\nThis can be generalized for any algorithm that is the composite of N independent differentially private algorithms:\n$$ \\tag{4.5} \\frac {Pr \\Big(\\mathbb{C}(D_1) = (r_1, r_2, \u0026hellip; r_n) ) \\Big)} {Pr \\Big(\\mathbb{C}(D_2) = (r_1, r_2, \u0026hellip; r_n) \\Big)} \\leq e ^ {\\epsilon_1 + \\epsilon_2 + \u0026hellip; + \\epsilon_n} $$\nThe sum of all privacy losses is the global privacy loss, which can be divided between the N algorithms as needed. The composability of DP tells us that theoretically we can assert that at most privacy erodes linearly with respect to n-epsilons, though in practice it is sublinear.\nAnd a bit more math‚Ä¶ Poking at Œµ Some reasoning about how to set ( \\(\\epsilon \\) ), and what it means for the analysis is readily developed by poking at Equation 02 with limits: Let \\(\\epsilon \\rightarrow 0\\) $$ \\tag{2} \\Bigg( \\frac {Pr\\Big(\\mathbb{A}(D_1) \\in S \\Big)} {Pr \\Big(\\mathbb{A}(D_2) \\in S \\Big)} \\leq { e ^ \\epsilon } \\Bigg) {\\Bigg\\vert _{\\epsilon \\rightarrow 0}} $$\n$$ \\tag{2.1} \\frac {Pr\\Big(\\mathbb{A}(D_1) \\in S \\Big)} {Pr \\Big(\\mathbb{A}(D_2) \\in S \\Big)} \\leq 1 $$\nFor the case where \\(\\epsilon \\rightarrow 0\\), for some given output in range \\(S\\) we are at most equally likely to deduce that the output has come from \\(D_1\\) or \\(D_2\\). This means that we have maximized the uncertainty as to which dataset informed the output. Within the context of the opening intuition: maximum uncertainty is maximum privacy. In Practice DP‚Äôs definition is really a conceptual property that describes a data transformation. There are a few potential transformations that satisfy this property, some popular mechanisms are additive noise mechanisms which sample noise from a distribution and inject that noise into the query response. Some popular additive noise mechanisms are:\nLaplace Mechanism This mechanism is the canonical DP mechanism that satisfies Œµ-DP (Equation 01), and is otherwise known as ‚Äòpure differential privacy‚Äô. This mechanism samples noise from a zero-centered Laplace distribution and injects this noise into the data release. 2\nGaussian Mechanism The Gaussian mechanism is very similar to the Laplace mechanism, except it uses a zero-centered, variance bound Guassian distribution and does not satisfy Œµ-DP, instead it satisfies (Œµ, Œ¥) Differential Privacy (Equation 03). In addition to restrictions on the Noise-Distribution‚Äôs variance, this additive noise mechanism only holds true to (Œµ, Œ¥) DP when œµ \u0026lt; 1.\nüî® Implementation tips Differential Privacy is a powerful tool, while wielding it watch for:\nDimensions in the source data that are correlated: this leads to leaky privacy guarantees Extreme outliers in the data: users in the long tails of data are more identifiable, require more noise to mask. Consider binning data into ranges to address this. This is also the subject of the case study DP with Privacy Budget Accounting Composition has been demonstrated for ŒµDP algorithms, which allows us to define a global limit on the maximum privacy loss that we will allow for a given dataset. When this global privacy loss is set, multiple queries are each assigned some portion of the whole budget. In other words, repeated queries deplete the global privacy budget.\nReferring back to Equation 02 - note that for lower Œµ values we expect more uncertainty in which of the datasets the algorithm‚Äôs output came from D1 or D2. As a generalization we have:\nLower Œµ value:\nHigher the noise we expect in the query response Lower accuracy in the query response as a result of the noise Better privacy story, but at the expense of the data utility üîß Œµ Setting Tips Fixing the global privacy budget means that care is needed when accounting for where to allocate higher accuracy queries, or where a sacrifice on accuracy may be made to stay within the budget.\nIn addition to careful accounting, cached answers can extend the budget. During the fulfillment of the query-response protocol: cache answers to queries, and retrieve cached answers when the same query is issued multiple times to the database. This type of response-replay won‚Äôt deplete the privacy budget, and ensures no additional information is learned about the data‚Äôs subjects for repeated queries.\nCase Study The following case study is hosted on GitHub, which includes a Creative Commons licensed dataset (thanks Kaggle!). In this case study we will be exploring the implementation of OpenMined PyDP which is a python wrapper that makes use of Google‚Äôs open source C++ Differential Privacy library. This Python library uses the Laplace mechanism of additive noise to satisfy ŒµDP.\nObjective: In this case study we will see the effect that:\nDifferent Œµ values have on the differential privacy noise Long tail skew in the source data have on differential privacy noise The Data For the fulfillment of this case study the data will first need to be:\nSanitized (e.g. correct string entries in numeric columns, etc) Columns dropped (the data cube diced) In addition to this clean up, we can drop all but the last month‚Äôs data for simplicity\u0026rsquo;s sake (data cube sliced). For the remainder of the case study we will be using the ‚ÄòAnnual_Income‚Äò dimension only.\nAnnual Income plotted in linear and log scale This is a terrific dimension to explore: it exhibits a rich (pun intended üí∞), long tailed right skew. Note that the tail is so long that it is impractical to plot annual income in linear scale (left), in log-scale we get a greater sense of where most users are and where only a few, very wealthy users are (wonder if they are hiring üòâ)\nThe Private Queries The Git Python project includes methods for private queries:\nRepeated_average Repeated_sum Repated_max Which take as input:\niterations (integer) number of times the private query will be repeated privacy_budget (float) epsilon value, does not change between iterations list (array) dataframe dimension to list, that is used as source data for the private query These methods do not use any privacy loss budget accounting, as a result over N-Iterations we expect a nice Laplace distribution in the observed query responses that should center on the true value.\nNoise \u0026amp; Œµ Running the repeated average method over 100 iterations, with Œµ = [1,100] returns the following histograms of observed query respones:\nRepeated Private Average Queries, 100 iterations with Epsilon = 100 Repeated Private Average Queries, 100 iterations with Epsilon = 1 As expected, a higher Œµ value yields query responses with lower noise injected - demonstrated by a distribution with a tighter spread, both distributions are centered on the true value which is represented by a vertical line.\nNoise \u0026amp; Long-tailed data We can imagine that blending in at a crowded event is much easier than trying to blend in in an otherwise empty room; so too there is a certain degree of privacy protection among a concentrated band of data rather than in a long tail. In other words, being a member of the 107 annual income club makes it difficult to apply enough noise to mask your contribution in the annual income dataset when running a private average query.\nRunning repeated private queries with iterations = 1000 and Œµ = 0.85 for the raw dataset (right skew, without any skew correction) and again for a skew-corrected dataset (still right skew, but cutting off anyone that isn‚Äôt below the 95% percentile).\nFor skew correction, we want to keep the count of unique users (in the dataset, count of rows) the same between datasets to make an apple to apple comparison. To address this, the skew-correction method takes any entry above 95 percentile, and replaces their contribution to annual income to be something within the sub-95 percentile band (using a random number generator within a bound).\nNormalized noise observations for repeated private queries, 1000 iterations, epsilon = 0.85 These histograms are normalized to their respective average salaries. Eliminating the long tail results in over a degree of magnitude change for the normalized noise distributions. üëÄ\nIrit Dinur and Kobbi Nissim, Revealing information while preserving privacy\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith, Calibrating noise to sensitivity in private data analysis\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKobbi Nissim , Thomas Steinke , Alexandra Wood, Micah Altman, Aaron Bembenek, Mark Bun, Marco Gaboardi, David R. O‚ÄôBrien, and Salil Vadhan, Differential Privacy: A Primer for a Non-technical Audience\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor, Our data, Ourselves: Privacy via Distributions Noise Generation\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://dlm.rocks/posts/differential_privacy_laplace_01/","summary":"\u003cp\u003eThis post serves as a primer to Differential Privacy: presenting an intuitive foundation for the definition, proceeding to some math, and finally presenting a case study to demonstrate some key concepts.\u003c/p\u003e","title":"Differential Privacy: Primer \u0026 Laplace Case Study"},{"content":"In this inaugural post I walk through the decision making that informed the design of this site (which tools and technologies I used), and the steps to set it all up.\nüìå TL;DR This site is\nhosted on Firebase created using Hugo has a custom domain purchased on Google Domains Built following these steps Lessons / Key takeaways Start projects with a goal, list of requirements and constraints. Then consider the options before starting on any work. I knew I wanted to set up my own site, because I‚Äôve been wanting to learn the basics of front end dev and have a space to play around in At first, I started using Octopress for content generation then double backed to research alternatives before landing on Hugo. If I had researched first, I would have saved some time. This site is hosted on Firebase It has a free tier with plenty of headroom before I hit any price point Offers a nice runway to grow the site beyond static content, with serverless backends and plenty of plugins to try in the future For custom domains purchased on Google Domains, the ‚Äòownership check‚Äô is skipped which saves time and implementation complexity The content is generated using Hugo Hugo has an active community (reddit, discord, and thorough documentation) which is reassuring for a FE beginner It‚Äôs written in Go, which is also something on my #ToLearn list Planning the build I followed these steps to get this site up:\nStart with a goal, and list of requirements and constraints Generate the site content Find a host and custom domain First: Goals, requirements, and constraints TL;DR If you want to focus on writing, there is no need to set up your own site. Go with Medium, which is the best free-to-post site IMO.\nI had intended to learn some FE basics + start writing this year. Setting up this blog allowed me to peel two potatoes at the same time (cruelty free figures of speech only on this site üê¶ üê¶)\nGoals Find a place to host my writing Learn HTML, Javascript, and CSS basics Host in a place that allows me to link to github projects Stretch: Run code from the site itself Non-goals Build a dynamic site (\u0026hellip;yet) / build anything that would necessitate a database Requirements I‚Äôm new to front end development, so I need something that comes with a framework I can tinker with as opposed to building something from scratch The framework should come with an active community so that I can find support as needed I am not interested in opening my home network to the internet, so I‚Äôd like to host on a third party platform Preferably a free-to-host platform Constraints Time: My primary goal is to have a place to write, but my secondary goal is having a sandbox to explore and configure a site. A tension arises here as the tinkering pulls time out of writing, and vice versa. Second, generate the content TL;DR I chose Hugo because it had the most active community for support and it‚Äôs written in Go which is a language I wanted to pick up anyway.\nI went on the hunt for static content generators, which are tools to create the html, css, and javascript that would become my site. In doing so, I found a new requirement that the site be tolerable aesthetically so a generator with an active community and support for themes was added to my list from the requirements step.\nI found this great list of site generators, from there I narrowed down to a few options based on the technologies used as well as the communities supporting the tooling:\nSite Content Options Description Comments Eleventy Open sourced Javascript static site generator Handles incremental builds Has an active Git, good documentation, and active Discord Many actively maintained plugins Requires a bit more setup for hosting on GitHub Pages \u0026 Firebase (Anecdotally) Next.js Minimalist Jamstack framework using Javascript, typescript, and rust. Objective is to build scalable React web apps (static, dynamic) Good community, popular modern framework Lack of native themes Lacks plugins (e.g. 'read time' was a feature I was after on my site) Steep learning curve compared to plug and play generators Hugo Written in Go General purpose static site framework Handles incremental builds Fastest builder (thanks Go!) Limited plugins Many community supported themes Steep learning curve to create new themes (thanks Go :frown:) but many themes already to choose from. I went with Hugo, because it offered a quick setup with plenty of room for tinkering after the initialization. It‚Äôs also written in Go which makes site building fast for quick prototyping down the road.\nThird, finding a host TL;DR I went with Firebase which offers trivial custom domain setup along with a free tier that offers plenty of headroom for the traffic and site content I plan to create.\nI considered the following for hosting - I initially published to GitHub Pages; which seemingly offered the fastest time-to-deployment since my source was in a public repo anyway, but switched to Firebase after configuring my custom domain. Firebase, I found, offered the most runway for continued development beyond static content only (no concrete plans here, but the optionality is valuable).\nHost Options Description Comments Netlify Serverless Jamstack platform. Free tier up to 300 build minutes per month. Support for web dev CI/CD Many 3P service APIs 1-click roll backs Github Pages Git repo hosting service that may run the files through a jekyll build process. Soft build limit (10/hour), and site size (1gb served, 100gb bandwidth) Git provided cert üîí Zero-config build Firebase App and rich site development platform, offers automated backend services and a suite of features for release and app measurement. Has a free tier also Zero-config SSL certs üîí Trivial setup for real-time backend 1-click roll backs GCP backed for serving at scale (GCP comes with $300 new acct credits) Impressive open-sourced bundled features NOTE: Not an exhaustive list, see more complete list here.\nFinally, getting a neat custom domain Pricing was a wash and there is no differentiator in the product offering itself (no matter the seller, you‚Äôre getting the same domain!) I went with Google Domains because when adding the custom domain in Firebase, if purchased from Google Domains then the ownership proof is done automatically which will save some time and implementation compleixty.\nNOTE: GitHub Pages threw an error when I tried to reconfigure the build to my custom domain, so I chose to relocate my site to Firebase at this point.\nBuilding it I will skip steps for hosting on GitHub Pages, which can otherwise be found here, and provide the steps I followed to host on Firebase (including setting up my custom domain name on Google Domains)\nPrerequisites: Git, Hugo, and Node.js\nbrew install git brew install hugo brew install node Setup a Github repo for the site\nCreate a repo with the naming convention [username].github.io\nI left ‚Äúmain‚Äù as the source for Hugo build and created a separate branch for the site called gh-pages NOTE: this was initially for the github pages deployment, but I kept the naming convention.\nClone the github repo locally.\nyou‚Äôll want to be working locally from the main branch Initialize Hugo in your cloned repo.\nhugo new site [username].github.io/ cd [username].github.io/ Picking a theme requires downloading \u0026amp; updating the config file. I am using PaperMod\nNOTE: PaperMod instructions use a YAML based Hugo build but I kept TOML. You can find YAML ‚Üí TOML translators online. A sample YAML config for PaperMod is here\ngit clone https://github.com/adityatelange/hugo-PaperMod themes/PaperMod --depth=1 configure only the contents of ./public/* (the built site) to push to the gh-pages branch mkdir .github/ .github/workflows touch .github/workflows/ Add the following content to that file:\nname: github pages on: push: branches: - main # Set a branch to deploy pull_request: jobs: deploy: runs-on: ubuntu-22.04 steps: - uses: actions/checkout@v3 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#39;latest\u0026#39; # extended: true - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 if: github.ref == \u0026#39;refs/heads/main\u0026#39; with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_dir: ./public Now, the local working copy is ready for prototyping!\nCreate a new post and view it locally to test Hugo and your new theme.\nhugo new posts/HelloWorld.md echo \u0026#34;Hello World!\u0026#34; \u0026gt;\u0026gt; posts/HelloWorld.md hugo server -D Pro-Tip: you can draft content in Google Docs, then use a Markdown converter extension called ‚ÄúDocs to Markdown‚Äù. Otherwise working from and IDE does the trick also.\nSetting up a custom domain for Firebase project Create a Firebase project for the site Purchase an available domain, let‚Äôs say somenamehere.com/ Add custom domain to Firebase hosting In Firebase project console, select Hosting from left nav menu Under the project domains, select ‚ÄúAdd custom domains‚Äù Add an entry for www.somenamehere.com Add an entry for somenamehere.com, select ‚Äúredirect to an existing website‚Äù and redirect to www.somenamehere.com Note the IP addresses assigned at these steps, as well as the record types (e.g. ‚ÄúA‚Äù) Firebase HostingAdding custom domain to Firebase hosting\nRegister DNS, return to Google Domains From Google Domains console, select DNS from left nav menu Under ‚ÄúCustom Records‚Äù select ‚ÄúManage custom records‚Äù Enter in the custom domain somenamehere.com and www.somenamehere.com here along with the noted IP addresses and the record types. Google DomainsUpdating DNS Register\nInitializing Firebase and deploying the site Initialize Firebase at the root blog directory, in this example it‚Äôs been [username].github.io npm install -g firebase-tools firebase login # will be prompted for firebase login in browser firebase init In addition to the selections noted here,\nselect gh-pages as the source for the site during setup Now, the site can be deployed üöÄ\nhugo \u0026amp;\u0026amp; firebase deploy Don\u0026rsquo;t forget to push your built site to Git üíæ\n","permalink":"https://dlm.rocks/posts/how_this_all_started/","summary":"\u003cp\u003eIn this inaugural post I walk through the decision making that informed the design of this site (which tools and technologies I used), and the steps to set it all up.\u003c/p\u003e","title":"How this all started..."},{"content":"Welcome! üëã Hi, you\u0026rsquo;ve wandered to the about page.\nAbout the Site Welcome to David\u0026rsquo;s idea sharing site. Here you will find ramblings about topics like:\nprivacy \u0026amp; cybersecurity ML \u0026amp; data science Neat books Good podcast recommendations Maybe some artwork too *Note: topics list is growing and subject to change\nRecently, infrared radiation was recorded from the deepest pocket of space we\u0026rsquo;ve seen (albeit for a small slice) of our known Universe. It took in the order of 1010 years for this information to reach your computer screen, I commit to posting at least more often than it has taken for this deep space to reach you üëç\nDeep UniversePhoto by NASA‚Äôs James Webb Telescope\nA note on privacy \u0026hellip; just when you thought you\u0026rsquo;d get away without some privacy talk \u0026hellip;\nThis site uses Google Analytics, I use this data to understand:\nwhat content my audience is engaging with (e.g. which pages are visited) who my audience is, in coarse terms (e.g. coarse geotag) These data are used to measure content engagement.\nThis site does not\nhave a subscribe feature have log-in\u0026rsquo;s, comments, or other content submission forms include referrer in share links This site is also open source, so you can independently verify this üôå\nAbout the Author Greetings.\nI am David, born and raised in the Minneapolis, and currently living in California.\nI enjoy spending my time learning new things, spending time outdoors, and spending time with my family. Discovering activities that allow for all three at the same time is the sweet spot, if you got leads on this please let me know. Here is a fun pseudonymized pictured of myself next to an Alpaca.\nPixelated engineer \u0026amp; an alpaca, Circa 2019Engineer makes poor attempt at privacy joke in \u0026lsquo;about me\u0026rsquo; section\nIn my copious amounts of free time I\u0026rsquo;m excited to bring to you some neat stuff I know.\nWant more about me? I\u0026rsquo;ll keep it short:\nLikes Traveling and trying new foods Meeting people\u0026rsquo;s pets üê∂ üê±\u0026hellip; esp. those with human names Music, all kinds Dislikes lists without at least one item on them Thanks for checking out my blog / site. I hope you find some of this material educational, or at least entertaining.\nEnjoy your stay.\n- D\n","permalink":"https://dlm.rocks/about/","summary":"Welcome! üëã Hi, you\u0026rsquo;ve wandered to the about page.\nAbout the Site Welcome to David\u0026rsquo;s idea sharing site. Here you will find ramblings about topics like:\nprivacy \u0026amp; cybersecurity ML \u0026amp; data science Neat books Good podcast recommendations Maybe some artwork too *Note: topics list is growing and subject to change\nRecently, infrared radiation was recorded from the deepest pocket of space we\u0026rsquo;ve seen (albeit for a small slice) of our known Universe.","title":"About"}]